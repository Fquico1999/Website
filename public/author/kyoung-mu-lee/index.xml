<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kyoung Mu Lee | Francisco Farinha</title>
    <link>https://Fquico1999.github.io/author/kyoung-mu-lee/</link>
      <atom:link href="https://Fquico1999.github.io/author/kyoung-mu-lee/index.xml" rel="self" type="application/rss+xml" />
    <description>Kyoung Mu Lee</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2024 Francisco Farinha | [Academic](https://sourcethemes.com/academic/) | [Hugo](https://gohugo.io)</copyright><lastBuildDate>Fri, 26 Jun 2020 10:37:12 -0700</lastBuildDate>
    <image>
      <url>https://Fquico1999.github.io/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_512x512_fill_lanczos_center_3.png</url>
      <title>Kyoung Mu Lee</title>
      <link>https://Fquico1999.github.io/author/kyoung-mu-lee/</link>
    </image>
    
    <item>
      <title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
      <link>https://Fquico1999.github.io/post/vdsr_paper/</link>
      <pubDate>Fri, 26 Jun 2020 10:37:12 -0700</pubDate>
      <guid>https://Fquico1999.github.io/post/vdsr_paper/</guid>
      <description>&lt;p&gt;Find the paper 
&lt;a href=&#34;https://arxiv.org/abs/1511.04587&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;at-a-glance&#34;&gt;At a Glance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The paper proposes an effective model for single image super resolution that is highly accurate.&lt;/li&gt;
&lt;li&gt;Increasing the model depth increases overall accuracy.&lt;/li&gt;
&lt;li&gt;Contextual information over large regions is built up by cascading multiple smaller filters.&lt;/li&gt;
&lt;li&gt;Convergence speed is maximized by learning only residuals, and using large learning rates with adjustable gradient clipping.&lt;/li&gt;
&lt;li&gt;May be usefull in denoising and compression artifact removal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of the paper is to introduce a single image super resolution (SISR) model that addresses some of the limitations of a previously proposed framework, the SRCNN.&lt;/p&gt;
&lt;p&gt;The advantages of using CNNs for super resolution is that they provide an effective end-to-end solution, whereas past work required hand-engineered features.&lt;/p&gt;
&lt;p&gt;The paper lists three limitations of SRCNNs and how VDSR can address these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN is context dependent in small images&lt;/em&gt; - Information in a small patch does not hold enough information for detail recovery. VDSR addresses this by cascading small filters to capture large region information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Training for deep CNNs is slow&lt;/em&gt; - VDSR addresses this by only learning &lt;em&gt;residuals&lt;/em&gt; - the difference between the Low Resolution (LR) and High Resolution (HR) images. This works because the LR and HR images share the same information to a very large extent. Additionally, very large learning rates are used during training, with adjustable gradient clipping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN only works for a single scale&lt;/em&gt; - A single VDSR model is adequate for multi-scale-factor super resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-method&#34;&gt;Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;proposed-network&#34;&gt;Proposed Network&lt;/h3&gt;
&lt;p&gt;The network takes in an interpolated LR (ILR) image of shape $w \times h \times 3$ and predicts the residual image ($ w \times h \times 1$) which is then added onto the ILR to yield the HR image ($w \times h \times 3$).&lt;/p&gt;
&lt;p&gt;The network is comprised of $L$ layers where all but $l=1,20$ (first and last) follow ZEROPAD -&amp;gt; CONV($3\times 3, 64 \text{ filters}$) -&amp;gt; RELU. The first layer operates on the input and the last layer consists of ZEROPAD -&amp;gt; CONV($3\times 3, 1 \text{ filter}$) to output the desired residual image.&lt;/p&gt;
&lt;p&gt;The purpose of zero-padding before each convolution is to preserve the size of the feature maps. One issue with deep CNNs is that the convolution operation reduces the size of the feature map. Pixels on the border cannot be inferred properly, so usually SISR methods crop the boundary out which is fine for shallow models, but for deep CNNs it is unfeasible. Zero-padding addresses this issue, and is reported to work well.&lt;/p&gt;
&lt;p&gt;$L$ is specified to be $20$ in the paper&amp;rsquo;s training description.&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;The Loss function was the mean squared error averaged over the training set: $\frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2$, where $\pmb{y}$ is the HR image corresponding to the input LR image, and $\pmb{\hat{y}}$ is the model predicted HR image.&lt;/p&gt;
&lt;h4 id=&#34;residual-learning&#34;&gt;Residual Learning&lt;/h4&gt;
&lt;p&gt;The residual image is defined as $\pmb{r}=\pmb{y}-\pmb{x}$. Most values are likely to be small or zero, which is desirable when training. Since we want the network to predict the residual $\pmb{r}$, the loss function can be rewritten as $\frac{1}{2}  || \pmb{r} - \pmb{\hat{y}}||^2$. However, in the actual network training, the loss is the $L_2$ norm betweeen the reconstructed image $\pmb{r}+\pmb{x}$ and the ground truth $\pmb{y}$.&lt;/p&gt;
&lt;p&gt;Mini-batch Gradient Descent was used with a momentum optimizer (I assume, as the paper references momentum $\beta = 0.9$, could also be the Adam optimizer) and a weight decay of $0.0001$ (weight decay means adding a regularizing term to the loss, $\mathcal{L} = \frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2 + \gamma L_2, \gamma=0.0001$)&lt;/p&gt;
&lt;h4 id=&#34;adjustable-gradient-clipping&#34;&gt;Adjustable Gradient Clipping&lt;/h4&gt;
&lt;p&gt;An issue when training deep CNNs is the slow speed of convergence. One tactic to speed up training is to increase the learning rate $\alpha$, however this can lead to exploding gradients.&lt;/p&gt;
&lt;p&gt;One solution to this is referred to as Gradient Clipping where the gradients of the parameters with respect to the loss function are clipped between a certain range $[-\theta, \theta]$. The issue with this approach is that, at the start of training when the learning rate is very high, $\theta$ must be very small to prevent exploding gradients, however as the network is trained, learning rate is annealed and as such $\alpha \frac{\partial{\mathcal{L}}}{\partial{W}}$ gets increasingly smaller.&lt;/p&gt;
&lt;p&gt;The suggested method is to set gradients between $[-\frac{\theta}{\alpha}, \frac{\theta}{\alpha}]$, so the clipping is adjusted based on the current learning rate.&lt;/p&gt;
&lt;h4 id=&#34;multi-scale&#34;&gt;Multi-Scale&lt;/h4&gt;
&lt;p&gt;The model can be adapted to handle mutliple scales by simply training it on data of varying scales.
Images are divided into sub-images without overlap where sub-images from different scales are present.&lt;/p&gt;
&lt;p&gt;The paper tests the performance of a model trained with $s_{train}=\{2\}$ (scale factor of 2 in the training set) on different input scales and sees that for $s_{train} \ne s_{test}$, performance is bad. However when $s_{train}=\{2,3,4\}$ the performance at each scale factor is comparable with a corresponding single-scale network, even outperforming single-scale models at large scales (3,4).&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;VDSR outperforms Bicubic, A+, RFL, SelfEx, and SRCNN (all methods listed) in every regard (PSNR/SSIM/time).&lt;/p&gt;
&lt;p&gt;Benchmarks were made on Set5, Set14, B100 and Urban100 datasets.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;After reading the paper, I decided to implement VDSR in Keras. Please note this is a very quick-and-dirty implementation, it forgoes the adjustable gradient clipping and the learning rate adjustments made in the paper.&lt;/p&gt;
&lt;p&gt;I also test the model on one of the classes in the CIFAR10 dataset, namely the frog class.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, ZeroPadding2D, ReLU, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.backend import resize_images
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdsr(input_dim, l):
    
    #Define input layer
    LR = Input(shape=input_dim, name=&#39;input&#39;)
    
    #First convolution
    X = ZeroPadding2D()(LR)
    X = Conv2D(64,(3,3), name=&#39;CONV1&#39;)(X)
    X = ReLU()(X)
    
    #Repeat convolution layers untill last layer
    for i in range(l-2):
        X = ZeroPadding2D()(X)
        X = Conv2D(64, (3,3), name=&#39;CONV%i&#39; % (i+2))(X)
        X = ReLU()(X)
    
    #Final layer, output is residual image
    X = ZeroPadding2D()(X)
    residual = Conv2D(1, (3,3), name=&#39;CONV%i&#39; % l)(X)
    
    #Add residual to LR
    out = Add()([LR, residual])
    
    return Model(LR, out)
        
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Load the cifar10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
classes = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;,&#39;dog&#39;,&#39;frog&#39;, &#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;]

#Example image
i = np.random.randint(x_train.shape[0])

plt.imshow(x_train[i])
plt.axis(&#39;off&#39;)
plt.title(classes[y_train[i][0]], color=&#39;w&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_2_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#I&#39;ll use just the frog classes
train_idx = (y_train == [classes.index(&#39;frog&#39;)])

y_train = x_train[np.squeeze(train_idx)]

test_idx = (y_test == [classes.index(&#39;frog&#39;)])

y_test = x_test[np.squeeze(test_idx)]

print(&amp;quot;Training set ground truth has shape: &amp;quot; + str(y_train.shape))

print(&amp;quot;Test set ground truth has shape: &amp;quot; + str(y_test.shape))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training set ground truth has shape: (5000, 32, 32, 3)
Test set ground truth has shape: (1000, 32, 32, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w , h = y_train.shape[1:-1]
scale_factor = 2

x_train = []
for img in y_train:
    img_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)
    x_train.append(img_re)

x_train = np.asarray(x_train)

x_test = []
for img in y_test:
    img_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)
    #Normalize
    x_test.append(img_re)

x_test = np.asarray(x_test)

print(&amp;quot;Training set has shape: &amp;quot; + str(x_train.shape))

print(&amp;quot;Test set has shape: &amp;quot; + str(x_test.shape))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training set has shape: (5000, 16, 16, 3)
Test set has shape: (1000, 16, 16, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Example sample and label
i = np.random.randint(x_train.shape[0])

fig, ax = plt.subplots(1,2)
fig.set_size_inches(12,6)
fig.set_facecolor(&#39;w&#39;)

ax[0].imshow(x_train[i])
ax[0].set_title(&#39;X&#39;, fontsize=14)
ax[0].axis(&#39;off&#39;)

ax[1].imshow(y_train[i])
ax[1].set_title(&#39;Y&#39;, fontsize=14)
ax[1].axis(&#39;off&#39;)

plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Define the model
input_dim = y_train.shape[1:]
L = 20
model = vdsr(input_dim, L)
model.compile(optimizer=Adam(learning_rate=0.000075,beta_1=0.9), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Need to upscale input
x_train_scaled = resize_images(x_train/255.0, scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)
x_test_scaled = resize_images(x_test/255.0, scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)


history = model.fit(x_train_scaled, y_train/255., batch_size=64, epochs=10, validation_data=(x_test_scaled, y_test/255.0))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 5000 samples, validate on 1000 samples
Epoch 1/10
5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0030 - accuracy: 0.9441 - val_loss: 0.0028 - val_accuracy: 0.9451
Epoch 2/10
5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0027 - accuracy: 0.9441 - val_loss: 0.0027 - val_accuracy: 0.9451
Epoch 3/10
5000/5000 [==============================] - 139s 28ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 4/10
5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 5/10
5000/5000 [==============================] - 135s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 6/10
5000/5000 [==============================] - 142s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 7/10
5000/5000 [==============================] - 132s 26ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 8/10
5000/5000 [==============================] - 140s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 9/10
5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 10/10
5000/5000 [==============================] - 130s 26ms/sample - loss: 0.0024 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
fig.set_size_inches(12,12)
fig.set_facecolor(&#39;w&#39;)

ax.plot(history.history[&#39;loss&#39;], label=&#39;loss&#39;)
ax.plot(history.history[&#39;val_loss&#39;], label=&#39;val_loss&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Example sample and label
i = np.random.randint(x_train.shape[0])

x_i = resize_images(np.expand_dims(x_train[i],axis=0), scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)

y_hat = model.predict(x_i/255.0)[0]

fig, ax = plt.subplots(1,3)
fig.set_size_inches(12,6)
fig.set_facecolor(&#39;w&#39;)

ax[0].imshow(x_train[i])
ax[0].set_title(&#39;X&#39;, fontsize=14)
ax[0].axis(&#39;off&#39;)

ax[1].imshow(y_train[i])
ax[1].set_title(&#39;Y&#39;, fontsize=14)
ax[1].axis(&#39;off&#39;)

ax[2].imshow(y_hat)
ax[2].set_title(&#39;Y_hat&#39;, fontsize=14)
ax[2].axis(&#39;off&#39;)

plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The model seems to work well and converges quickly, even for a fixed learning rate. Of note is that the CIFAR10 images are of size $32\times 32 \times 3$, so to test super resolution with a scale factor of 2, the input images are resized from $16 \times 16 \times 3$. Compared to Set5&amp;rsquo;s $256 \times 256 \times 3$ images used in the paper, this test isn&amp;rsquo;t reflective of VDSR&amp;rsquo;s ability to infer detail.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
