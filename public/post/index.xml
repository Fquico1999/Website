<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Francisco Farinha</title>
    <link>https://Fquico1999.github.io/post/</link>
      <atom:link href="https://Fquico1999.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2024 Francisco Farinha | [Academic](https://sourcethemes.com/academic/) | [Hugo](https://gohugo.io)</copyright><lastBuildDate>Thu, 02 Jul 2020 22:51:27 -0700</lastBuildDate>
    <image>
      <url>https://Fquico1999.github.io/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://Fquico1999.github.io/post/</link>
    </image>
    
    <item>
      <title>A Neural Algorithm of Artistic Style</title>
      <link>https://Fquico1999.github.io/post/nst/</link>
      <pubDate>Thu, 02 Jul 2020 22:51:27 -0700</pubDate>
      <guid>https://Fquico1999.github.io/post/nst/</guid>
      <description>&lt;p&gt;Find the paper 
&lt;a href=&#34;https://arxiv.org/abs/1508.06576&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNN) are powerful in image processing related tasks. They consist of layers of small computational units that process visual information in a heirachical fashion. Each layer is essentially a collection of image filters that extract a certain feature from the input image.&lt;/p&gt;
&lt;p&gt;When trained on tasks such as image detection, CNNs develop image representations that are increasingly explicit along the processing path. In other words, the input image is successively transformed into representations that increasingly care about content of the image instead of local pixel values.&lt;/p&gt;
&lt;p&gt;The paper refers to feature respnses in higher layers of the network as the content representation.&lt;/p&gt;
&lt;p&gt;To obtain a meaningful representation of the style of an input image, the authors utilize a texture feature space. Essentially, by taking the filter responses in each layer of the networks and taking correlations between them over the channels in the feature maps, an objective method of texture evaluation is constructed.&lt;/p&gt;
&lt;p&gt;Reconstructions from style features produce versions of the input that are texturized and capture its general appearance in color and local structure. The size and complexity increases along the hierarchy, due to increasing feature complexity and receptive field size. This representation is denoted as style representation.&lt;/p&gt;
&lt;p&gt;The key finding in the paper is that content and style representations in CNNs are separable.&lt;/p&gt;
&lt;p&gt;Novel images can be synthesized by finding an image that can simultaneously match the content representation of an input photo as well as the style representation of a respective piece of art.&lt;/p&gt;
&lt;p&gt;The most visually appealing images are created by matching the style representation including up to the highest layers in the network, whereas style can be defined more locally by only including a lower number of layers.&lt;/p&gt;
&lt;p&gt;When synthesizing style and content, both must be included. As such, the loss function that is minimized includes components for both content and style with hyperparameters that allow to tweak emphasis on either.&lt;/p&gt;
&lt;p&gt;The authors derived these content and style representations from feature responses of DNNs trained on object recognition.&lt;/p&gt;
&lt;p&gt;Rendering a photo in the style of a certain artwork is approached by the computer vision field of non-photorealistic rendering. These methods relied on non-parametric techniques that manipulate the pixel representations directly. DNNs, on the other hand, manipulate feature spaces that represent style and content.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;The paper&amp;rsquo;s results were based on the VGG-Network, VGG19 I believe.&lt;/p&gt;
&lt;p&gt;The authors disregard the fully connected layers, and only focus on the 16 CONV and 5 POOL layers to build the feature space.&lt;/p&gt;
&lt;p&gt;Instead of Max Pooling, the authors found that Average Pooling worked better - it improves gradient flow and the results become more visually appealing.&lt;/p&gt;
&lt;p&gt;Let the input image be denoted as $\vec{x}$.&lt;/p&gt;
&lt;p&gt;Each layer of the VGG Network beign considered has $N_l$ filters of size $M_l = W_l \times H_l$.&lt;/p&gt;
&lt;p&gt;Responses of layer $l$ are stored in $F^{l}$, where $F^{l} \in \mathcal{R}^{N_l \times M_l}$. We index $F$ as $F^{l}_{ij}$ which is the activation of filter $i$ at position $j$ in $M^l$ of layer $l$.&lt;/p&gt;
&lt;p&gt;Gradient Descent is performed on a white noise image to find one that matches the feature responses of the original image.&lt;/p&gt;
&lt;p&gt;Given original image, $\vec{p}$, and generated image, $\vec{x}$,  with feature representations at layer $l$ given by $P^{l}$ and $F^{l}$ respectively, the content loss is the squared-error loss between the feature representations:&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{content}(\vec{p},\vec{x},l) = \frac{1}{2}\sum_{ij}(F^{l}_{ij} - P^{l}_{ij})^2$$&lt;/p&gt;
&lt;p&gt;So, the derivative of the loss with respect to the activations is:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathcal{L}_{content}}{\partial  F^{l}_{ij}} = \left\{
\begin{array}{ll}
(F^l-P^l)_{ij} &amp;amp; \text{if} F^{l}_{ij} &amp;gt; 0\\
0 &amp;amp; \text{if} F^{l}_{ij} &amp;lt; 0
\end{array}
\right.$$&lt;/p&gt;
&lt;p&gt;Then, we change $\vec{x}$ untill it generates the same response in a certain layer as $\vec{p}$.&lt;/p&gt;
&lt;p&gt;The style representation is built by taking the correlations between different filter responses. These correlations are given by Gram matrix $G^{l} \in \mathcal{R}^{N_l \times N_l}$. Here, $G^{l}_{ij}$ is the inner product between feature maps $i,j$ in layer $l$:&lt;/p&gt;
&lt;p&gt;$$G^{l}_{ij} = \sum_{k} F^{l}_{ik}F^{l}_{jk}$$&lt;/p&gt;
&lt;p&gt;Once more, we use Gradient Descent from a white noise image to find another image that matches the style representation. Like the content representation, we minimize the mean-squared distance between entries of the Gram matrix of the original image and the Gram matrix of the generated image.&lt;/p&gt;
&lt;p&gt;Given original image, $\vec{a}$, and generated image, $\vec{x}$, with style representations at layer $l$ given by $A^{l}$ and $G^{l}$ respectively, the loss from a single layer $l$ is:&lt;/p&gt;
&lt;p&gt;$$E_l = \frac{1}{4N^{2}_{l}M^{2}_{l}} \sum_{ij}(G^{l}_{ij}-A^{l}_{ij})^2$$&lt;/p&gt;
&lt;p&gt;With total loss:&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{style}(\vec{a},\vec{x}) = \sum^{L}_{l=0} w_lE_l$$&lt;/p&gt;
&lt;p&gt;with $w_l$ weighting factors of the contribution of each layer. The derivatice of $E_l$ with respect to activations in layer $l$ becomes:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial E_l}{\partial  F^{l}_{ij}} = \left\{
\begin{array}{ll}
\frac{1}{N_l^2 M_l^2}((F^l)^T (G^l-A^l))_{ji} &amp;amp; \text{if} F^{l}_{ij} &amp;gt; 0\\
0 &amp;amp; \text{if} F^{l}_{ij} &amp;lt; 0
\end{array}
\right.$$
To generate images that mix the content of a photo with the style of a painting, we need to minimize the distance of a white noise image to the content representation of the photo in a layer as well as the style representtion of the paining over a number of layers. Given photograph $\vec{p}$ and artwork $\vec{a}$, the loss function becomes:&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{total}(\vec{p}, \vec{a}, \vec{x}) =  \alpha \mathcal{L}_{content}(\vec{p}, \vec{x}) + \beta \mathcal{L}_{style}(\vec{a},\vec{x})$$&lt;/p&gt;
&lt;p&gt;With $\alpha, \beta$ weighting factors to control content and style reconstruction respectively.&lt;/p&gt;
&lt;p&gt;In the paper, content representation was matched for layer &lt;code&gt;conv4_2&lt;/code&gt; and the style representation used layers &lt;code&gt;conv1_1&lt;/code&gt;, &lt;code&gt;conv2_1&lt;/code&gt;, &lt;code&gt;conv3_1&lt;/code&gt;, &lt;code&gt;conv4_1&lt;/code&gt;, and &lt;code&gt;conv5_1&lt;/code&gt;, with $w_l = \frac{1}{5}$ in those layers and $w_l=0$ in all others. In general, $w_l = \frac{1}{N_a}$ with $N_a$ being the number of active layers.&lt;/p&gt;
&lt;p&gt;Additionally, the paper uses $\frac{\alpha}{\beta} \in \{1\times 10^{-3},1 \times 10^{-4}\}$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
      <link>https://Fquico1999.github.io/post/vdsr_paper/</link>
      <pubDate>Fri, 26 Jun 2020 10:37:12 -0700</pubDate>
      <guid>https://Fquico1999.github.io/post/vdsr_paper/</guid>
      <description>&lt;p&gt;Find the paper 
&lt;a href=&#34;https://arxiv.org/abs/1511.04587&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;at-a-glance&#34;&gt;At a Glance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The paper proposes an effective model for single image super resolution that is highly accurate.&lt;/li&gt;
&lt;li&gt;Increasing the model depth increases overall accuracy.&lt;/li&gt;
&lt;li&gt;Contextual information over large regions is built up by cascading multiple smaller filters.&lt;/li&gt;
&lt;li&gt;Convergence speed is maximized by learning only residuals, and using large learning rates with adjustable gradient clipping.&lt;/li&gt;
&lt;li&gt;May be usefull in denoising and compression artifact removal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of the paper is to introduce a single image super resolution (SISR) model that addresses some of the limitations of a previously proposed framework, the SRCNN.&lt;/p&gt;
&lt;p&gt;The advantages of using CNNs for super resolution is that they provide an effective end-to-end solution, whereas past work required hand-engineered features.&lt;/p&gt;
&lt;p&gt;The paper lists three limitations of SRCNNs and how VDSR can address these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN is context dependent in small images&lt;/em&gt; - Information in a small patch does not hold enough information for detail recovery. VDSR addresses this by cascading small filters to capture large region information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Training for deep CNNs is slow&lt;/em&gt; - VDSR addresses this by only learning &lt;em&gt;residuals&lt;/em&gt; - the difference between the Low Resolution (LR) and High Resolution (HR) images. This works because the LR and HR images share the same information to a very large extent. Additionally, very large learning rates are used during training, with adjustable gradient clipping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN only works for a single scale&lt;/em&gt; - A single VDSR model is adequate for multi-scale-factor super resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-method&#34;&gt;Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;proposed-network&#34;&gt;Proposed Network&lt;/h3&gt;
&lt;p&gt;The network takes in an interpolated LR (ILR) image of shape $w \times h \times 3$ and predicts the residual image ($ w \times h \times 1$) which is then added onto the ILR to yield the HR image ($w \times h \times 3$).&lt;/p&gt;
&lt;p&gt;The network is comprised of $L$ layers where all but $l=1,20$ (first and last) follow ZEROPAD -&amp;gt; CONV($3\times 3, 64 \text{ filters}$) -&amp;gt; RELU. The first layer operates on the input and the last layer consists of ZEROPAD -&amp;gt; CONV($3\times 3, 1 \text{ filter}$) to output the desired residual image.&lt;/p&gt;
&lt;p&gt;The purpose of zero-padding before each convolution is to preserve the size of the feature maps. One issue with deep CNNs is that the convolution operation reduces the size of the feature map. Pixels on the border cannot be inferred properly, so usually SISR methods crop the boundary out which is fine for shallow models, but for deep CNNs it is unfeasible. Zero-padding addresses this issue, and is reported to work well.&lt;/p&gt;
&lt;p&gt;$L$ is specified to be $20$ in the paper&amp;rsquo;s training description.&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;The Loss function was the mean squared error averaged over the training set: $\frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2$, where $\pmb{y}$ is the HR image corresponding to the input LR image, and $\pmb{\hat{y}}$ is the model predicted HR image.&lt;/p&gt;
&lt;h4 id=&#34;residual-learning&#34;&gt;Residual Learning&lt;/h4&gt;
&lt;p&gt;The residual image is defined as $\pmb{r}=\pmb{y}-\pmb{x}$. Most values are likely to be small or zero, which is desirable when training. Since we want the network to predict the residual $\pmb{r}$, the loss function can be rewritten as $\frac{1}{2}  || \pmb{r} - \pmb{\hat{y}}||^2$. However, in the actual network training, the loss is the $L_2$ norm betweeen the reconstructed image $\pmb{r}+\pmb{x}$ and the ground truth $\pmb{y}$.&lt;/p&gt;
&lt;p&gt;Mini-batch Gradient Descent was used with a momentum optimizer (I assume, as the paper references momentum $\beta = 0.9$, could also be the Adam optimizer) and a weight decay of $0.0001$ (weight decay means adding a regularizing term to the loss, $\mathcal{L} = \frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2 + \gamma L_2, \gamma=0.0001$)&lt;/p&gt;
&lt;h4 id=&#34;adjustable-gradient-clipping&#34;&gt;Adjustable Gradient Clipping&lt;/h4&gt;
&lt;p&gt;An issue when training deep CNNs is the slow speed of convergence. One tactic to speed up training is to increase the learning rate $\alpha$, however this can lead to exploding gradients.&lt;/p&gt;
&lt;p&gt;One solution to this is referred to as Gradient Clipping where the gradients of the parameters with respect to the loss function are clipped between a certain range $[-\theta, \theta]$. The issue with this approach is that, at the start of training when the learning rate is very high, $\theta$ must be very small to prevent exploding gradients, however as the network is trained, learning rate is annealed and as such $\alpha \frac{\partial{\mathcal{L}}}{\partial{W}}$ gets increasingly smaller.&lt;/p&gt;
&lt;p&gt;The suggested method is to set gradients between $[-\frac{\theta}{\alpha}, \frac{\theta}{\alpha}]$, so the clipping is adjusted based on the current learning rate.&lt;/p&gt;
&lt;h4 id=&#34;multi-scale&#34;&gt;Multi-Scale&lt;/h4&gt;
&lt;p&gt;The model can be adapted to handle mutliple scales by simply training it on data of varying scales.
Images are divided into sub-images without overlap where sub-images from different scales are present.&lt;/p&gt;
&lt;p&gt;The paper tests the performance of a model trained with $s_{train}=\{2\}$ (scale factor of 2 in the training set) on different input scales and sees that for $s_{train} \ne s_{test}$, performance is bad. However when $s_{train}=\{2,3,4\}$ the performance at each scale factor is comparable with a corresponding single-scale network, even outperforming single-scale models at large scales (3,4).&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;VDSR outperforms Bicubic, A+, RFL, SelfEx, and SRCNN (all methods listed) in every regard (PSNR/SSIM/time).&lt;/p&gt;
&lt;p&gt;Benchmarks were made on Set5, Set14, B100 and Urban100 datasets.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;After reading the paper, I decided to implement VDSR in Keras. Please note this is a very quick-and-dirty implementation, it forgoes the adjustable gradient clipping and the learning rate adjustments made in the paper.&lt;/p&gt;
&lt;p&gt;I also test the model on one of the classes in the CIFAR10 dataset, namely the frog class.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, ZeroPadding2D, ReLU, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.backend import resize_images
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdsr(input_dim, l):
    
    #Define input layer
    LR = Input(shape=input_dim, name=&#39;input&#39;)
    
    #First convolution
    X = ZeroPadding2D()(LR)
    X = Conv2D(64,(3,3), name=&#39;CONV1&#39;)(X)
    X = ReLU()(X)
    
    #Repeat convolution layers untill last layer
    for i in range(l-2):
        X = ZeroPadding2D()(X)
        X = Conv2D(64, (3,3), name=&#39;CONV%i&#39; % (i+2))(X)
        X = ReLU()(X)
    
    #Final layer, output is residual image
    X = ZeroPadding2D()(X)
    residual = Conv2D(1, (3,3), name=&#39;CONV%i&#39; % l)(X)
    
    #Add residual to LR
    out = Add()([LR, residual])
    
    return Model(LR, out)
        
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Load the cifar10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
classes = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;,&#39;dog&#39;,&#39;frog&#39;, &#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;]

#Example image
i = np.random.randint(x_train.shape[0])

plt.imshow(x_train[i])
plt.axis(&#39;off&#39;)
plt.title(classes[y_train[i][0]], color=&#39;w&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_2_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#I&#39;ll use just the frog classes
train_idx = (y_train == [classes.index(&#39;frog&#39;)])

y_train = x_train[np.squeeze(train_idx)]

test_idx = (y_test == [classes.index(&#39;frog&#39;)])

y_test = x_test[np.squeeze(test_idx)]

print(&amp;quot;Training set ground truth has shape: &amp;quot; + str(y_train.shape))

print(&amp;quot;Test set ground truth has shape: &amp;quot; + str(y_test.shape))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training set ground truth has shape: (5000, 32, 32, 3)
Test set ground truth has shape: (1000, 32, 32, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w , h = y_train.shape[1:-1]
scale_factor = 2

x_train = []
for img in y_train:
    img_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)
    x_train.append(img_re)

x_train = np.asarray(x_train)

x_test = []
for img in y_test:
    img_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)
    #Normalize
    x_test.append(img_re)

x_test = np.asarray(x_test)

print(&amp;quot;Training set has shape: &amp;quot; + str(x_train.shape))

print(&amp;quot;Test set has shape: &amp;quot; + str(x_test.shape))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training set has shape: (5000, 16, 16, 3)
Test set has shape: (1000, 16, 16, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Example sample and label
i = np.random.randint(x_train.shape[0])

fig, ax = plt.subplots(1,2)
fig.set_size_inches(12,6)
fig.set_facecolor(&#39;w&#39;)

ax[0].imshow(x_train[i])
ax[0].set_title(&#39;X&#39;, fontsize=14)
ax[0].axis(&#39;off&#39;)

ax[1].imshow(y_train[i])
ax[1].set_title(&#39;Y&#39;, fontsize=14)
ax[1].axis(&#39;off&#39;)

plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Define the model
input_dim = y_train.shape[1:]
L = 20
model = vdsr(input_dim, L)
model.compile(optimizer=Adam(learning_rate=0.000075,beta_1=0.9), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Need to upscale input
x_train_scaled = resize_images(x_train/255.0, scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)
x_test_scaled = resize_images(x_test/255.0, scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)


history = model.fit(x_train_scaled, y_train/255., batch_size=64, epochs=10, validation_data=(x_test_scaled, y_test/255.0))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 5000 samples, validate on 1000 samples
Epoch 1/10
5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0030 - accuracy: 0.9441 - val_loss: 0.0028 - val_accuracy: 0.9451
Epoch 2/10
5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0027 - accuracy: 0.9441 - val_loss: 0.0027 - val_accuracy: 0.9451
Epoch 3/10
5000/5000 [==============================] - 139s 28ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 4/10
5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 5/10
5000/5000 [==============================] - 135s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451
Epoch 6/10
5000/5000 [==============================] - 142s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 7/10
5000/5000 [==============================] - 132s 26ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 8/10
5000/5000 [==============================] - 140s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 9/10
5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
Epoch 10/10
5000/5000 [==============================] - 130s 26ms/sample - loss: 0.0024 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
fig.set_size_inches(12,12)
fig.set_facecolor(&#39;w&#39;)

ax.plot(history.history[&#39;loss&#39;], label=&#39;loss&#39;)
ax.plot(history.history[&#39;val_loss&#39;], label=&#39;val_loss&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Example sample and label
i = np.random.randint(x_train.shape[0])

x_i = resize_images(np.expand_dims(x_train[i],axis=0), scale_factor, scale_factor,&#39;channels_last&#39;, interpolation=&#39;bilinear&#39;)

y_hat = model.predict(x_i/255.0)[0]

fig, ax = plt.subplots(1,3)
fig.set_size_inches(12,6)
fig.set_facecolor(&#39;w&#39;)

ax[0].imshow(x_train[i])
ax[0].set_title(&#39;X&#39;, fontsize=14)
ax[0].axis(&#39;off&#39;)

ax[1].imshow(y_train[i])
ax[1].set_title(&#39;Y&#39;, fontsize=14)
ax[1].axis(&#39;off&#39;)

ax[2].imshow(y_hat)
ax[2].set_title(&#39;Y_hat&#39;, fontsize=14)
ax[2].axis(&#39;off&#39;)

plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./VDSR_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The model seems to work well and converges quickly, even for a fixed learning rate. Of note is that the CIFAR10 images are of size $32\times 32 \times 3$, so to test super resolution with a scale factor of 2, the input images are resized from $16 \times 16 \times 3$. Compared to Set5&amp;rsquo;s $256 \times 256 \times 3$ images used in the paper, this test isn&amp;rsquo;t reflective of VDSR&amp;rsquo;s ability to infer detail.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
