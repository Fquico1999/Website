[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m Francisco, a Data Scientist at BlackBerry who is passionate about tackling real world problems with the exciting potential of Machine Learning. Currently, my interests are in model security and differential privacy, as well as anomaly-based IDS.\nWhile you\u0026rsquo;re here, check out some of my projects, view my resum√©, or drop me a line below.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Fquico1999.github.io/author/francisco-farinha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/francisco-farinha/","section":"authors","summary":"I\u0026rsquo;m Francisco, a Data Scientist at BlackBerry who is passionate about tackling real world problems with the exciting potential of Machine Learning. Currently, my interests are in model security and differential privacy, as well as anomaly-based IDS.","tags":null,"title":"Francisco Farinha","type":"authors"},{"authors":["Leon A. Gatys","Alexander S. Ecker","Matthias Bethge"],"categories":[],"content":"Find the paper here\nConvolutional Neural Networks (CNN) are powerful in image processing related tasks. They consist of layers of small computational units that process visual information in a heirachical fashion. Each layer is essentially a collection of image filters that extract a certain feature from the input image.\nWhen trained on tasks such as image detection, CNNs develop image representations that are increasingly explicit along the processing path. In other words, the input image is successively transformed into representations that increasingly care about content of the image instead of local pixel values.\nThe paper refers to feature respnses in higher layers of the network as the content representation.\nTo obtain a meaningful representation of the style of an input image, the authors utilize a texture feature space. Essentially, by taking the filter responses in each layer of the networks and taking correlations between them over the channels in the feature maps, an objective method of texture evaluation is constructed.\nReconstructions from style features produce versions of the input that are texturized and capture its general appearance in color and local structure. The size and complexity increases along the hierarchy, due to increasing feature complexity and receptive field size. This representation is denoted as style representation.\nThe key finding in the paper is that content and style representations in CNNs are separable.\nNovel images can be synthesized by finding an image that can simultaneously match the content representation of an input photo as well as the style representation of a respective piece of art.\nThe most visually appealing images are created by matching the style representation including up to the highest layers in the network, whereas style can be defined more locally by only including a lower number of layers.\nWhen synthesizing style and content, both must be included. As such, the loss function that is minimized includes components for both content and style with hyperparameters that allow to tweak emphasis on either.\nThe authors derived these content and style representations from feature responses of DNNs trained on object recognition.\nRendering a photo in the style of a certain artwork is approached by the computer vision field of non-photorealistic rendering. These methods relied on non-parametric techniques that manipulate the pixel representations directly. DNNs, on the other hand, manipulate feature spaces that represent style and content.\nMethods The paper\u0026rsquo;s results were based on the VGG-Network, VGG19 I believe.\nThe authors disregard the fully connected layers, and only focus on the 16 CONV and 5 POOL layers to build the feature space.\nInstead of Max Pooling, the authors found that Average Pooling worked better - it improves gradient flow and the results become more visually appealing.\nLet the input image be denoted as $\\vec{x}$.\nEach layer of the VGG Network beign considered has $N_l$ filters of size $M_l = W_l \\times H_l$.\nResponses of layer $l$ are stored in $F^{l}$, where $F^{l} \\in \\mathcal{R}^{N_l \\times M_l}$. We index $F$ as $F^{l}_{ij}$ which is the activation of filter $i$ at position $j$ in $M^l$ of layer $l$.\nGradient Descent is performed on a white noise image to find one that matches the feature responses of the original image.\nGiven original image, $\\vec{p}$, and generated image, $\\vec{x}$, with feature representations at layer $l$ given by $P^{l}$ and $F^{l}$ respectively, the content loss is the squared-error loss between the feature representations:\n$$\\mathcal{L}_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2}\\sum_{ij}(F^{l}_{ij} - P^{l}_{ij})^2$$\nSo, the derivative of the loss with respect to the activations is:\n$$\\frac{\\partial \\mathcal{L}_{content}}{\\partial F^{l}_{ij}} = \\left\\{ \\begin{array}{ll} (F^l-P^l)_{ij} \u0026amp; \\text{if} F^{l}_{ij} \u0026gt; 0\\\\ 0 \u0026amp; \\text{if} F^{l}_{ij} \u0026lt; 0 \\end{array} \\right.$$\nThen, we change $\\vec{x}$ untill it generates the same response in a certain layer as $\\vec{p}$.\nThe style representation is built by taking the correlations between different filter responses. These correlations are given by Gram matrix $G^{l} \\in \\mathcal{R}^{N_l \\times N_l}$. Here, $G^{l}_{ij}$ is the inner product between feature maps $i,j$ in layer $l$:\n$$G^{l}_{ij} = \\sum_{k} F^{l}_{ik}F^{l}_{jk}$$\nOnce more, we use Gradient Descent from a white noise image to find another image that matches the style representation. Like the content representation, we minimize the mean-squared distance between entries of the Gram matrix of the original image and the Gram matrix of the generated image.\nGiven original image, $\\vec{a}$, and generated image, $\\vec{x}$, with style representations at layer $l$ given by $A^{l}$ and $G^{l}$ respectively, the loss from a single layer $l$ is:\n$$E_l = \\frac{1}{4N^{2}_{l}M^{2}_{l}} \\sum_{ij}(G^{l}_{ij}-A^{l}_{ij})^2$$\nWith total loss:\n$$\\mathcal{L}_{style}(\\vec{a},\\vec{x}) = \\sum^{L}_{l=0} w_lE_l$$\nwith $w_l$ weighting factors of the contribution of each layer. The derivatice of $E_l$ with respect to activations in layer $l$ becomes:\n$$\\frac{\\partial E_l}{\\partial F^{l}_{ij}} = \\left\\{ \\begin{array}{ll} \\frac{1}{N_l^2 M_l^2}((F^l)^T (G^l-A^l))_{ji} \u0026amp; \\text{if} F^{l}_{ij} \u0026gt; 0\\\\ 0 \u0026amp; \\text{if} F^{l}_{ij} \u0026lt; 0 \\end{array} \\right.$$ To generate images that mix the content of a photo with the style of a painting, we need to minimize the distance of a white noise image to the content representation of the photo in a layer as well as the style representtion of the paining over a number of layers. Given photograph $\\vec{p}$ and artwork $\\vec{a}$, the loss function becomes:\n$$\\mathcal{L}_{total}(\\vec{p}, \\vec{a}, \\vec{x}) = \\alpha \\mathcal{L}_{content}(\\vec{p}, \\vec{x}) + \\beta \\mathcal{L}_{style}(\\vec{a},\\vec{x})$$\nWith $\\alpha, \\beta$ weighting factors to control content and style reconstruction respectively.\nIn the paper, content representation was matched for layer conv4_2 and the style representation used layers conv1_1, conv2_1, conv3_1, conv4_1, and conv5_1, with $w_l = \\frac{1}{5}$ in those layers and $w_l=0$ in all others. In general, $w_l = \\frac{1}{N_a}$ with $N_a$ being the number of active layers.\nAdditionally, the paper uses $\\frac{\\alpha}{\\beta} \\in \\{1\\times 10^{-3},1 \\times 10^{-4}\\}$\n","date":1593755487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593755487,"objectID":"25eb35e338dea70d86876379f74e147e","permalink":"https://Fquico1999.github.io/post/nst/","publishdate":"2020-07-02T22:51:27-07:00","relpermalink":"/post/nst/","section":"post","summary":"Creating artistic images using Deep Neural Networks","tags":[],"title":"A Neural Algorithm of Artistic Style","type":"post"},{"authors":[],"categories":[],"content":"You can find this repository here\nOverview The base concept of a GAN in this scenario is that we have two seperate neural networks which are trained differently.\nThere is a Discriminator and a Generator network with different inputs and outputs.\nThe discriminator classifies images as real or not real (In other words, as coming from the generated distribution or the real data distribution). In this case, the input is an image, and the output is a probability of the input belonging to the real dataset distribution.\nThe generator takes in random seeds and will output an image.\nTraining the Generator Both networks need to be implemented at the same time, but the weights must be updated at different times. To train the generator, we freeze the discriminator weights. We input just random seeds to the generator and the output images are fed into the discriminator. Which will try and evaluate whether they are generated or not. So the labels for this step are all $y_i=1.0$. Backpropagation trains the generator to produce images that better \u0026ldquo;fool\u0026rdquo; the discriminator\nTraining the Discriminator We freeze the weights of the Generator, we generate images with the generator, and we take the same number of images from the input data distribution. These are fed into the discriminator which outputs probabilities of being from the input dataset. Thus backpropagation will train the discriminator to be able to distinguish real from generated images.\nimport tensorflow as tf\rtf.enable_eager_execution()\rfrom tensorflow.keras.losses import BinaryCrossentropy\rfrom tensorflow.keras.optimizers import Adam\rfrom tensorflow.keras.models import Sequential, Model, load_model\rfrom tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, ZeroPadding2D, ReLU\rfrom tensorflow.keras.layers import BatchNormalization, Flatten, Dense, UpSampling2D\rfrom tensorflow.keras.layers import Reshape, Activation, Conv2DTranspose\rfrom tensorflow.keras.layers import AveragePooling2D, Input, Add\rfrom tensorflow.keras.backend import resize_images\rgpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\rsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r# config = tf.ConfigProto()\r# config.gpu_options.allow_growth = True\r# session = tf.Session(config=config)\rimport numpy as np\rimport csv\rimport time\rimport os\rimport matplotlib.pyplot as plt\rfrom PIL import Image\rfrom tqdm import tqdm\rimport dask.array as da\rDefining the Models We have two models to define, the generator and the discriminator.\nIntuitively, if the output we want to generate are images, then the discriminator will be a ConvNet, since it needs to extract features from an image to output a single numerical prediction. So it\u0026rsquo;s no surprise our model takes the image and applies a series of convolutions. To prevent overfitting, we also apply Dropout to our layers.\nOn the other hand, the generator takes in a vector seed and generates an image. So intuitively it must be composed of a series of upsamplings, which it is.\ndef get_discriminator(image_shape):\rmodel = Sequential()\rmodel.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding=\u0026quot;same\u0026quot;))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(Dropout(0.25))\rmodel.add(Conv2D(64, kernel_size=3, strides=2, padding=\u0026quot;same\u0026quot;))\rmodel.add(ZeroPadding2D(padding=((0,1),(0,1))))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(Dropout(0.25))\rmodel.add(Conv2D(128, kernel_size=3, strides=2, padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(Dropout(0.25))\rmodel.add(Conv2D(256, kernel_size=3, strides=1, padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(Dropout(0.25))\rmodel.add(Conv2D(512, kernel_size=3, strides=1, padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(Dropout(0.25))\rmodel.add(Flatten())\rmodel.add(Dense(1, activation='sigmoid'))\rreturn model\rdef get_generator(seed_size):\rmodel = Sequential()\rmodel.add(Dense(4*4*256,activation=\u0026quot;relu\u0026quot;,input_dim=seed_size))\rmodel.add(Reshape((4,4,256)))\rmodel.add(UpSampling2D())\rmodel.add(Conv2D(256,kernel_size=3,padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(UpSampling2D())\rmodel.add(Conv2D(256,kernel_size=3,padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\r# Output resolution, additional upsampling\rmodel.add(UpSampling2D())\rmodel.add(Conv2D(128,kernel_size=3,padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\rmodel.add(UpSampling2D(size=(2,2)))\rmodel.add(Conv2D(128,kernel_size=3,padding=\u0026quot;same\u0026quot;))\rmodel.add(BatchNormalization(momentum=0.8))\rmodel.add(LeakyReLU(alpha=0.2))\r# Final CNN layer\rmodel.add(Conv2D(3,kernel_size=3,padding=\u0026quot;same\u0026quot;))\rmodel.add(Activation(\u0026quot;tanh\u0026quot;))\rreturn model\rHelper Functions Here I\u0026rsquo;ll define functions that help with training the GAN.\nFirst, we need to define the loss function for both the discriminator and the generator since these are trained differently.\nI\u0026rsquo;ll also define a helper function to save progress images of the training\ndef loss_generator(Y_hat):\r\u0026quot;\u0026quot;\u0026quot;\rImplements Binary Crossentropy Loss for the Generator\rArguments:\rY_hat -- Discriminator Predictions\rReturns:\rloss -- BinaryCrossentropy loss for the generator\r\u0026quot;\u0026quot;\u0026quot;\r#Recall the generator is trained on y_hats of only one\rY = tf.ones_like(Y_hat)\rloss = BinaryCrossentropy(from_logits=True)(Y, Y_hat)\rreturn loss\rdef loss_discriminator(Y_hat_real, Y_hat_gen):\r\u0026quot;\u0026quot;\u0026quot;\rImplements BinaryCrossentropy loss for the Discriminator\rArguments:\rY_hat_real -- Predictions on real distribution samples\rY_hat_gen -- Predictions on generated samples\rReturns:\rtotal -- Combined Real and Generated loss of Discriminator\r\u0026quot;\u0026quot;\u0026quot;\rentropy = BinaryCrossentropy(from_logits=True)\rY_real = tf.ones_like(Y_hat_real)\rY_gen = tf.zeros_like(Y_hat_gen)\rloss_real = entropy(Y_real, Y_hat_real)\rloss_gen = entropy(Y_gen, Y_hat_gen)\rtotal = loss_real+loss_gen\rreturn total\rdef save_images(output_path, epoch, seed):\rimage_array = np.full((16 + (4 * (W+16)), 16 + (7 * (H+16)), 3), 255, dtype=np.uint8)\rgenerated_images = generator.predict(seed)\rgenerated_images = 0.5 * generated_images + 0.5\rimage_count = 0\rfor row in range(4):\rfor col in range(7):\rr = row * (W+16) + 16\rc = col * (H+16) + 16\rimage_array[r:r+W,c:c+H] = generated_images[image_count] * 255\rimage_count += 1\rif not os.path.exists(output_path):\ros.makedirs(output_path)\rfilename = os.path.join(output_path,\u0026quot;train-%s.png\u0026quot;%epoch)\rim = Image.fromarray(image_array)\rim.save(filename)\rDefining a Training step Tensorflow allows for precise control over what a training step is comprised of. tf.function takes a python function and converts it to a graph representation that Tensorflow can use to perform automatic differentiation efficiently. Additionally, it provides us the power to control how the discriminator and generator get updated.\nIt is interesting how this process is accomplished. tf.GradientTape() allows Tensorflow to keep track of the operations defined in the function and then apply automatic differentiation.\nI\u0026rsquo;ll list through the operations that a training step requires:\nGenerate a random seed to input to the Generator\rObtain a generated dataset from the generator given the input seed\rObtain predictions from the discriminator on the dataset obtained from the real distribution\rObtian predictions from the discriminator on the generated dataset\rObtain losses for both generator and discriminator\rUsing Automatic Differentiation, obtain the gradients for the generator and discriminator Apply Backpropagation using the gradients.\r@tf.function\rdef step(batch):\rX_real, Y_real = batch\rseed = tf.random.normal([X_real.shape[0], Y_real.shape[1]])\r#GradientTape - how tf does automatic differentiation.\rwith tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\rX_gen = generator(seed, training=True)\rY_hat_real = discriminator(X_real, training=True)\rY_hat_gen = discriminator(X_gen, training=True)\rgen_loss = loss_generator(Y_hat_gen)\rdisc_loss = loss_discriminator(Y_hat_real, Y_hat_gen)\rgenerator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\rdiscriminator_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\rgenerator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\rdiscriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\rreturn gen_loss, disc_loss\rdef train(dataset, epochs, seed_size):\rfixed_seed = np.random.normal(0, 1, (4 * 7, seed_size))\rstart = time.time()\rfor epoch in range(epochs):\repoch_start = time.time()\rgen_loss_list = []\rdisc_loss_list = []\rfor image_batch in dataset:\rt = step(image_batch)\rgen_loss_list.append(t[0])\rdisc_loss_list.append(t[1])\rg_loss = sum(gen_loss_list) / len(gen_loss_list)\rd_loss = sum(disc_loss_list) / len(disc_loss_list)\repoch_elapsed = time.time()-epoch_start\rprint ('Epoch %i, gen loss=%f,disc loss=%f \\t %f' % (epoch+1, g_loss, d_loss, epoch_elapsed))\rsave_images(output_path,epoch,fixed_seed )\relapsed = time.time()-start\rprint ('Training time: %f' % elapsed)\rPreparing the Dataset I\u0026rsquo;ll be training the GAN on Google\u0026rsquo;s Cartoon Set. The premise is to be able to generate good-enough looking faces with set categories to be able to play a randomized game of Guess Who.\nThe dataset is comprised of 10,000 randomly generated cartoon faces, each with .csv files containing the descriptive features.\nThe size of these images are $500\\times500$. This is too large for my current GPU setup, so I\u0026rsquo;ll shrink it down to $64\\times64$. Additionally, the .csv files hold set characteristics about the cartoon such as type of hair, eye color, etc. This allows us to create seeds to represent each of the cartoons.\ndataset_path = './cartoonset10k/'\r#Visualize one entry\rfiles = os.listdir(dataset_path)\ri = np.random.randint(len(files)//2)\r#Investigate one sample\rfile_name = files[i].split('.')[0]\r#Images are .png\rimage_path = os.path.join(dataset_path, file_name+'.png')\rcsv_path = os.path.join(dataset_path, file_name+'.csv')\rImage.open(image_path)\r#View csv file\rcharacteristics = []\rvariants = []\rtotal_variants = []\rwith open(csv_path) as f:\rreader = csv.reader(f)\rfor row in reader:\rcharacteristic, value, num = row\rcharacteristics.append(characteristic)\rvariants.append(float(value))\rtotal_variants.append(float(num))\runique_seed = np.asarray(variants)/(np.asarray(total_variants)/2)-1.0\rprint(\u0026quot;Charateristics are:\\n %s\u0026quot; % str(characteristics))\rprint(\u0026quot;Unique Seed:\\n %s\u0026quot; % str(unique_seed))\rCharateristics are:\r['eye_angle', 'eye_lashes', 'eye_lid', 'chin_length', 'eyebrow_weight', 'eyebrow_shape', 'eyebrow_thickness', 'face_shape', 'facial_hair', 'hair', 'eye_color', 'face_color', 'hair_color', 'glasses', 'glasses_color', 'eye_slant', 'eyebrow_width', 'eye_eyebrow_distance']\rUnique Seed:\r[-0.33333333 0. 0. -0.33333333 0. -0.57142857\r-0.5 0.42857143 0.86666667 -0.63963964 0.6 0.09090909\r0.2 0.5 -1. -0.33333333 0.33333333 -1. ]\rGiven the set number of characteristics, we can define unique seeds for each of the faces in our dataset. In doing so we can hope to create a structured latent space that allows us to tweak and generate images based on characteristics that we wish.\nOne aspect of note. Since the input images are PNG files they have 4 channels, RGBA, where the last one is the transparancy layer. This is useless to us, however it is not sufficient to just remove it as I found it produced artifacts near the borders of the face itself. Instead, we composite the image with a background, and can then safely remove the transparancy layer\ndef get_unique_seed(csv_path):\r\u0026quot;\u0026quot;\u0026quot;\rFunction to determine seed for a given sample in the dataset\rAttributes:\rcsv_path -- python string, path to the csv file\rReturns:\runique_seed -- numpy array, unique seed of image, length equal to number of features in the dataset\r\u0026quot;\u0026quot;\u0026quot;\rvariants = []\rtotal_variants = []\rwith open(csv_path) as f:\rreader = csv.reader(f)\rfor row in reader:\r_ , value, num = row\rvariants.append(float(value))\rtotal_variants.append(float(num))\runique_seed = np.asarray(variants)/(np.asarray(total_variants)/2)-1.0\rreturn unique_seed\rdef get_features(csv_path):\r\u0026quot;\u0026quot;\u0026quot;\rObtains list of feature for the dataset\rAttributes:\rcsv_path -- python string, path to the csv file\rReturns:\rfeatures -- python list, features of a sample in the dataset. Fixed for the datset\r\u0026quot;\u0026quot;\u0026quot;\rfeatures = []\rwith open(csv_path) as f:\rreader = csv.reader(f)\rfor row in reader:\rfeat ,_,_ = row\rfeatures.append(feat)\rreturn features\r#Set width and height\rW = 64\rH = 64\rX = []\rY = []\rX_path = './X.npy'\rY_path = './Y.npy'\rfor i,file_name in tqdm(enumerate(os.listdir(dataset_path)), desc='Reading Data'):\r#Ensure only look at the images, to avoid duplicates\rif '.png' in file_name:\rname = file_name.split('.')[0]\r#Images are .png\rimage_path = os.path.join(dataset_path, name+'.png')\rcsv_path = os.path.join(dataset_path, name+'.csv')\r#Get feautures\rif i == 0:\rfeatures = get_features(csv_path)\r#Get unique seed\rseed = get_unique_seed(csv_path)\r#Read and resize image\rpng = Image.open(image_path).resize((W,H),Image.ANTIALIAS)\rbackground = Image.new('RGBA', png.size, (0,0,0))\r#Create alpha composite to avoid artifacts\ralpha_composite = Image.alpha_composite(background, png)\rimg = np.asarray(alpha_composite)/127.5 - 1.0\r#Remove transparancy layer\rX.append(img[...,:3])\rY.append(seed)\r#Convert to np\rX = np.asarray(X)\rY = np.asarray(Y)\rX = np.reshape(X,(-1,W, H, 3))\rX = X.astype(np.float32)\r#Save\rnp.save(X_path,X)\rnp.save(Y_path, Y)\rprint('Done')\r#Visualize one entry of the dataset\ri = np.random.randint(X.shape[0])\rfig, ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rax.imshow(X[i])\rax.set_title(Y[i])\rplt.axis('off')\rplt.show()\rGAN Stability and Failure Modes Before we start training, it is noteworthy to mention just how difficult GANs are to train. This difficulty arises because we need to train both a generative and discriminative model at the same time where improvements in one model will impact the other.\nBecause of this dynamic system, GANs can outright fail to converge. As such, I found it necessary to learn more about GAN stability and failure modes.\nFirst, I\u0026rsquo;ll analyze what a Stable GAN should look like while training. Some best practices are:\nUse Leaky ReLU activations instead of ReLU, since it is often preferred to normalize inputs to be between $[-1,1]$, and ReLU will set any inputs less than $0$ to be $0$. Use a Kernel Size that is a factor of the stride Use hyperbolic tan (tanh) as the output layer activation Once training begins, a stable GAN will have a generator loss somewhere around $[1.0,2.0]$ or higher, whereas the discriminator should hover around $0.5-0.8$.\nAccuracy of the discriminator on both generated and real images should be around $0.7,0.8$\nPersonally, while training, I found that the initial training steps are crucial for stable training. I attempted several values of the learning rate parameter $\\alpha$ which kept halting training without converging. The issue with this is that I required a very small $\\alpha$ ($5.0\\cdot 10^{-6}$) to be able to overcome the initial \u0026ldquo;turbulence\u0026rdquo;, however after becoming stable the improvements made were very slow as a result.\nTraining the Model X_path = './X.npy'\rY_path = './Y.npy'\r#Load the data\rX = np.load(X_path)\rY = np.load(Y_path)\r#Shuffle data\ridx = np.random.permutation(range(X.shape[0]))\rX = X[idx]\rY = Y[idx]\r#Dataset parameters\rbatch_size = 8\r#Convert to unshuffled tensorflow dataset object\rdataset = tf.data.Dataset.from_tensor_slices((X,Y)).batch(batch_size)\rBoth the Generator and Discriminator will be optimized with Adam. Adam has three parameters that define it. It can be thought of combining Gradient Descent with RMSProp and Momentum. So it inherits the learning rate $\\alpha$ parameter, as well as $\\beta_1$ from momentum and $\\beta_2$ from RMSProp. For a more in depth look at different optimizers, you can reference my implementation of neural nets from scratch in NumPy here.\n#\rW = 64\rH = 64\routput_path = './figures'\r#Define seed size\rseed_size = Y.shape[1]\r#Get models\rgenerator = get_generator(seed_size)\rdiscriminator = get_discriminator(X.shape[1:])\r#Alpha and Beta1 may need tuning. Beta2 most likely does not\ralpha = 5.0e-6\rbeta_1 = 0.8\rbeta_2 = 0.999\r#Get optimizers\rgenerator_optimizer = Adam(learning_rate=alpha, beta_1=beta_1, beta_2=beta_2)\rdiscriminator_optimizer = Adam(learning_rate=alpha, beta_1=beta_1, beta_2=beta_2)\r#Define parameters\repochs = 500\rtrain(dataset, epochs, seed_size)\rWARNING:tensorflow:From /home/francisco/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\rWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\rInstructions for updating:\rUse tf.where in 2.0, which has the same broadcast rule as np.where\rEpoch 1, gen loss=0.679988,disc loss=1.362854 76.512750\rEpoch 2, gen loss=0.693136,disc loss=1.386321 72.690860\rEpoch 3, gen loss=0.693149,disc loss=1.386317 75.233255\rAt this point, I realized my hardware was poorly equiped to handle the training, especially since I had to set the learning rate so small. So I copied my code to Google Colab and used their GPU power to run the model for $500$ epochs. The Generator and Discriminator Losses are plotted below (I absentmindedly forgot to include a plot for $i=499$ and I didnt want to run the training for another 3 hours, so bear with me):\nAdditionally, for each epoch I saved a set of images from the same seed to see how training progressed over time, which is quite fascinating:\nExploring the Latent Space Initially, I had hoped that by feeding the generator feature vectors corresponding to the features outlined in the .csv files for each picture, the model would learn a very structured latent space that would be easy to navigate. However, I found the model to have a very large bias when I did this, so I had to opt for random seeds instead to obtain the results shown above.\nAs such, We need to explore this latent space to verify if there are ways of tweaking the output. If there are (hopefully orthogonalized) methods of accomplishing this, It will allow us to control the outputs we get, and be able to generate faces for our game of Guess Who.\nThe seeds fed into the generator during training matched the number of features described in each .csv file, so they have dimension $1\\times18$, taking values in the range $(-1,1)$\n#Path to latest model (generator model that is)\rmodel_path = './models/generator_500.h5'\r#Load generator\rgenerator = load_model(model_path, compile=False)\r#Inference on random 1x18 vector\rX_i = tf.random.normal([1,18])\r#Get image, in range (-1,1)\ry_hat = generator.predict(X_i)[0]\r#Process image to be (0,255)\ry = ((y_hat+1.0)*127.5).astype(np.int32)\rfig,ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rax.imshow(y)\rax.set_title(X_i.numpy())\rplt.axis('off')\rplt.show()\rThe image above is an example of inference done on the model. It actually looks quite a bit better than the images in the time-lapse above because its been rescaled to be in the range $(0,255)$ as intended.\nMy initial strategy to explore the latent space will be to have a baseline prediction of just zeros, and vary one feature at a time. I am not expecting the latent space to be orthogonalized like that, but it might give some indication of the changes that occur.\n#Inference on zeros 1x18 vector\rX_i = tf.zeros([1,18])\r#Get image, in range (-1,1)\ry_hat = generator.predict(X_i)[0]\r#Process image to be (0,255)\ry = ((y_hat+1.0)*127.5).astype(np.int32)\rfig,ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rax.imshow(y)\rax.set_title(X_i.numpy())\rplt.axis('off')\rplt.show()\r#Start with the inital row\rnum_samples = 10\rX = np.zeros([num_samples, 18])\rsamples = np.linspace(-1,1,num=num_samples)\rX[:,0] = samples\rY = generator.predict(X)\rfig, ax = plt.subplots(1,num_samples)\rfig.set_size_inches(num_samples*2,12)\rfig.set_facecolor('w')\rfor i in range(num_samples):\ry_hat = Y[i]\ry = ((y_hat+1.0)*127.5).astype(np.int32)\rax[i].imshow(y)\rax[i].set_title(\u0026quot;%.02f\u0026quot; % samples[i])\rax[i].axis('off')\rfig.tight_layout()\rplt.show()\r#Make a finer sampling for gif\rnum_samples = 100\rX = np.zeros([num_samples, 18])\rsamples = np.linspace(-1,1,num=num_samples)\rX[:,0] = samples\rY = generator.predict(X)\rfig, ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rfor i in range(num_samples):\ry_hat = Y[i]\ry = ((y_hat+1.0)*127.5).astype(np.int32)\rax.imshow(y)\rax.axis('off')\rplt.savefig('./figures/feature_1/frame_%i' % i)\rThis is absolutely incredible! The latent space seems smooth. We can see, however, it is not orthogonalized, since by changing this parameter we change not only the hair style, but color and skin color as well.\nThe next step is to do the same for every feature.\nnum_samples = 10\rnum_feats = 18\rX = np.zeros([num_samples, num_feats])\rsamples = np.linspace(-1,1,num=num_samples)\rfig, ax = plt.subplots(num_feats,num_samples)\rfig.set_size_inches(num_samples*2,num_feats*2)\rfig.set_facecolor('w')\rfor i in range(num_feats):\rX_i = np.copy(X)\rX_i[:,i] = samples\rY = generator.predict(X_i)\rfor j in range(num_samples):\ry_hat = Y[j]\ry = ((y_hat+1.0)*127.5).astype(np.int32)\rax[i,j].imshow(y)\rax[i,j].set_title(\u0026quot;Feature %i : %.02f\u0026quot; % (i+1,samples[j]))\rax[i,j].axis('off')\rfig.tight_layout()\rplt.savefig('./figures/latentspace.png',dpi=200)\rplt.show()\rSuper Resolution / Upsampling Given the very large dataset of detailed images, we had to downscale the images to $64\\times64$ to be able to fit in memory during training.\nNow, I will train a super resolution model to upsample the images and obtain more detailed faces. I will be using the original dataset, in hopes that the GAN has learned a reasonable approximation to that distribution.\nNow, the original $500x500$ scale may be too large for adequate mapping. I\u0026rsquo;ll try with $256\\times256$.\nI\u0026rsquo;ll use a model architecture called VDSR, which stands for Very Deep Super Resolution. I review the paper here.\ndataset_path = './cartoonset10k/'\r#Set input width and height\rW_in = 64\rH_in = 64\r#Set output width and height\rW_out = 256\rH_out = 256\rX = []\rY = []\rX_path = './super_res_X.npy'\rY_path = './super_res_Y.npy'\rfiles = os.listdir(dataset_path)\ridx = np.random.randint(len(files), size=int(0.15*len(files)))\rfor i,file_name in tqdm(enumerate(np.asarray(files)[idx]), desc='Reading Data'):\r#Ensure only look at the images, to avoid duplicates\rif '.png' in file_name:\rname = file_name.split('.')[0]\r#Images are .png\rimage_path = os.path.join(dataset_path, name+'.png')\r#Read and resize image\rpng_in = Image.open(image_path).resize((W_in,H_in),Image.ANTIALIAS)\rbackground_in = Image.new('RGBA', png_in.size, (0,0,0))\r#Create alpha composite to avoid artifacts\ralpha_composite = Image.alpha_composite(background_in, png_in)\rimg_in = np.asarray(alpha_composite)/127.5 - 1.0\r#Get label\rpng_out = Image.open(image_path).resize((W_out, H_out), Image.BICUBIC)\rbackground_out = Image.new('RGBA', png_out.size, (0,0,0))\r#Create alpha composite to avoid artifacts\ralpha_composite = Image.alpha_composite(background_out, png_out)\rimg_out = np.asarray(alpha_composite)/127.5-1.0\rX.append(img_in)\rY.append(img_out)\r#Convert to np\rX = np.asarray(X)\rY = np.asarray(Y)\rX = np.reshape(X[...,:3],(-1,W_in, H_in, 3))\rX = X.astype(np.float32)\rY = np.reshape(Y[...,:3],(-1,W_out, H_out, 3))\rY = Y.astype(np.float32)\r#Save\r#np.save(X_path,X)\r#np.save(Y_path, Y)\rprint('Done')\rReading Data: 3000it [00:21, 138.79it/s]\rDone\rdef super_res_model(input_shape, output_shape):\rX_in = Input(shape = input_shape)\rX = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(X_in)\rX = Activation('relu')(X)\rfor _ in range(18):\rX = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(X)\rX = Activation('relu')(X)\r#Final layer for residual image\rX = Conv2D(1, (3, 3), padding='same', kernel_initializer='he_normal')(X)\rX = Add()([X, X_in])\rmodel = Model(inputs = X_in, outputs = X)\rreturn model\r#Set input width and height\rW_in = 64\rH_in = 64\r#Set output width and height\rW_out = 256\rH_out = 256\r#Resize input images with bicubic interpolation\rh_factor = H_out/H_in\rw_factor = W_out/ W_in\rX_resized = resize_images(X, h_factor, w_factor,data_format='channels_last',interpolation='bilinear')\r# model = super_res_model(X.shape[1:], Y.shape[1:])\r# model.compile(optimizer='adam',loss='mean_absolute_error', metrics=['accuracy'])\r# model.summary()\rlr = 0.000007\repochs = 75\rmodel = super_res_model(X_resized.shape.as_list()[1:], Y.shape[1:])\rmodel.compile(optimizer=Adam(lr=lr),loss='mean_absolute_error', metrics=['accuracy'])\rmodel.summary()\rModel: \u0026quot;model\u0026quot;\r__________________________________________________________________________________________________\rLayer (type) Output Shape Param # Connected to ==================================================================================================\rinput_1 (InputLayer) [(None, 256, 256, 3) 0 __________________________________________________________________________________________________\rconv2d (Conv2D) (None, 256, 256, 64) 1792 input_1[0][0] __________________________________________________________________________________________________\ractivation (Activation) (None, 256, 256, 64) 0 conv2d[0][0] __________________________________________________________________________________________________\rconv2d_1 (Conv2D) (None, 256, 256, 64) 36928 activation[0][0] __________________________________________________________________________________________________\ractivation_1 (Activation) (None, 256, 256, 64) 0 conv2d_1[0][0] __________________________________________________________________________________________________\rconv2d_2 (Conv2D) (None, 256, 256, 64) 36928 activation_1[0][0] __________________________________________________________________________________________________\ractivation_2 (Activation) (None, 256, 256, 64) 0 conv2d_2[0][0] __________________________________________________________________________________________________\rconv2d_3 (Conv2D) (None, 256, 256, 64) 36928 activation_2[0][0] __________________________________________________________________________________________________\ractivation_3 (Activation) (None, 256, 256, 64) 0 conv2d_3[0][0] __________________________________________________________________________________________________\rconv2d_4 (Conv2D) (None, 256, 256, 64) 36928 activation_3[0][0] __________________________________________________________________________________________________\ractivation_4 (Activation) (None, 256, 256, 64) 0 conv2d_4[0][0] __________________________________________________________________________________________________\rconv2d_5 (Conv2D) (None, 256, 256, 64) 36928 activation_4[0][0] __________________________________________________________________________________________________\ractivation_5 (Activation) (None, 256, 256, 64) 0 conv2d_5[0][0] __________________________________________________________________________________________________\rconv2d_6 (Conv2D) (None, 256, 256, 64) 36928 activation_5[0][0] __________________________________________________________________________________________________\ractivation_6 (Activation) (None, 256, 256, 64) 0 conv2d_6[0][0] __________________________________________________________________________________________________\rconv2d_7 (Conv2D) (None, 256, 256, 64) 36928 activation_6[0][0] __________________________________________________________________________________________________\ractivation_7 (Activation) (None, 256, 256, 64) 0 conv2d_7[0][0] __________________________________________________________________________________________________\rconv2d_8 (Conv2D) (None, 256, 256, 64) 36928 activation_7[0][0] __________________________________________________________________________________________________\ractivation_8 (Activation) (None, 256, 256, 64) 0 conv2d_8[0][0] __________________________________________________________________________________________________\rconv2d_9 (Conv2D) (None, 256, 256, 64) 36928 activation_8[0][0] __________________________________________________________________________________________________\ractivation_9 (Activation) (None, 256, 256, 64) 0 conv2d_9[0][0] __________________________________________________________________________________________________\rconv2d_10 (Conv2D) (None, 256, 256, 64) 36928 activation_9[0][0] __________________________________________________________________________________________________\ractivation_10 (Activation) (None, 256, 256, 64) 0 conv2d_10[0][0] __________________________________________________________________________________________________\rconv2d_11 (Conv2D) (None, 256, 256, 64) 36928 activation_10[0][0] __________________________________________________________________________________________________\ractivation_11 (Activation) (None, 256, 256, 64) 0 conv2d_11[0][0] __________________________________________________________________________________________________\rconv2d_12 (Conv2D) (None, 256, 256, 64) 36928 activation_11[0][0] __________________________________________________________________________________________________\ractivation_12 (Activation) (None, 256, 256, 64) 0 conv2d_12[0][0] __________________________________________________________________________________________________\rconv2d_13 (Conv2D) (None, 256, 256, 64) 36928 activation_12[0][0] __________________________________________________________________________________________________\ractivation_13 (Activation) (None, 256, 256, 64) 0 conv2d_13[0][0] __________________________________________________________________________________________________\rconv2d_14 (Conv2D) (None, 256, 256, 64) 36928 activation_13[0][0] __________________________________________________________________________________________________\ractivation_14 (Activation) (None, 256, 256, 64) 0 conv2d_14[0][0] __________________________________________________________________________________________________\rconv2d_15 (Conv2D) (None, 256, 256, 64) 36928 activation_14[0][0] __________________________________________________________________________________________________\ractivation_15 (Activation) (None, 256, 256, 64) 0 conv2d_15[0][0] __________________________________________________________________________________________________\rconv2d_16 (Conv2D) (None, 256, 256, 64) 36928 activation_15[0][0] __________________________________________________________________________________________________\ractivation_16 (Activation) (None, 256, 256, 64) 0 conv2d_16[0][0] __________________________________________________________________________________________________\rconv2d_17 (Conv2D) (None, 256, 256, 64) 36928 activation_16[0][0] __________________________________________________________________________________________________\ractivation_17 (Activation) (None, 256, 256, 64) 0 conv2d_17[0][0] __________________________________________________________________________________________________\rconv2d_18 (Conv2D) (None, 256, 256, 64) 36928 activation_17[0][0] __________________________________________________________________________________________________\ractivation_18 (Activation) (None, 256, 256, 64) 0 conv2d_18[0][0] __________________________________________________________________________________________________\rconv2d_19 (Conv2D) (None, 256, 256, 1) 577 activation_18[0][0] __________________________________________________________________________________________________\radd (Add) (None, 256, 256, 3) 0 conv2d_19[0][0] input_1[0][0] ==================================================================================================\rTotal params: 667,073\rTrainable params: 667,073\rNon-trainable params: 0\r__________________________________________________________________________________________________\rhistory = model.fit(X_resized.numpy(),Y,batch_size=1,epochs=60,validation_split=0.10)\rmodel_name = 'super_res_%f_%i' % (lr, epochs)\rmodel.save('/content/gdrive/My Drive/Colab Notebooks/GAN/' + model_name + '.h5')\rfig, ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rax.plot(history.history['accuracy'], label = 'accuracy')\rax.plot(history.history['val_accuracy'], label = 'validation_accuracy')\rax2 = ax.twinx()\rax2.plot(history.history['loss'], label = 'mse_loss')\rax2.plot(history.history['val_loss'], label = 'validation_mse_loss')\rplt.legend()\rplt.savefig('/content/gdrive/My Drive/Colab Notebooks/GAN/' + model_name+'_history.png')\rplt.show()\rUnfortunately, my poor computer wasn\u0026rsquo;t able to load the large tensors into RAM, even only using 20% of the dataset. So I trained the model in Google collab once more.\nHere is a sample input from that model\nEstabishing the Pipeline So we have two trained models. The first one is the generator that takes in a vector of noise and outputs an image of dimension $64\\times 64\\times 3$.\nWe also have a trained VDSR model that takes in images of dimension $256\\times 256\\times 3$ and outputs images of dimension $256\\times 256\\times 3$.\nThe inputs to the VDSR model are bicubic resizes of the $64\\times 64\\times 3$ outputs.\n#Path to latest generator model\rgen_model_path = './models/generator_500.h5'\r#Path to super resolution model\rres_model_path = './models/super_res_0.000007_75.h5'\r#Load generator\rgenerator = load_model(gen_model_path, compile=False)\r#Load super resolution model\rsuper_res_model = load_model(res_model_path, compile=False)\r## Params\r#Set input width and height\rW_in = 64\rH_in = 64\r#Set output width and height\rW_out = 256\rH_out = 256\r#Resize factor\rh_factor = H_out/H_in\rw_factor = W_out/ W_in\r## Generator\r#Inference on random 1x18 vector\rX_i = tf.random.normal([1,18])\r#Get image, in range (-1,1)\ry_hat = generator.predict(X_i)[0]\r#Process image to be (0,255)\ry = ((y_hat+1.0)*127.5).astype(np.int32)\r## Super Resolution\r#Resize\ry_resized = resize_images(np.expand_dims(y_hat, axis=0), h_factor, w_factor,data_format='channels_last',interpolation='bilinear')\r#Get super resolution prediction\rsuper_res_hat = super_res_model.predict(y_resized)[0]\r#Process image to be (0,255)\rsuper_res = ((super_res_hat+1.0)*127.5).astype(np.int32)\r## Plotting\rfig,ax = plt.subplots(1,2)\rfig.set_size_inches(24,12)\rfig.set_facecolor('w')\rax[0].imshow(y)\rax[0].set_title(X_i.numpy())\rax[1].imshow(super_res)\rax[1].set_title('Super Resolution')\rplt.axis('off')\rplt.show()\r","date":1593204017,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593204017,"objectID":"a09b118502d37ab890ca993d3ba73b64","permalink":"https://Fquico1999.github.io/project/ganwho/","publishdate":"2020-06-26T13:40:17-07:00","relpermalink":"/project/ganwho/","section":"project","summary":"Guess Who implementation with GAN generated and VDSR upscaled images.","tags":["Deep Learning"],"title":"GAN Who","type":"project"},{"authors":["Jiwon Kim","Jung Kwon Lee","Kyoung Mu Lee"],"categories":[],"content":"Find the paper here\nAt a Glance The paper proposes an effective model for single image super resolution that is highly accurate. Increasing the model depth increases overall accuracy. Contextual information over large regions is built up by cascading multiple smaller filters. Convergence speed is maximized by learning only residuals, and using large learning rates with adjustable gradient clipping. May be usefull in denoising and compression artifact removal Introduction The goal of the paper is to introduce a single image super resolution (SISR) model that addresses some of the limitations of a previously proposed framework, the SRCNN.\nThe advantages of using CNNs for super resolution is that they provide an effective end-to-end solution, whereas past work required hand-engineered features.\nThe paper lists three limitations of SRCNNs and how VDSR can address these:\nSRCNN is context dependent in small images - Information in a small patch does not hold enough information for detail recovery. VDSR addresses this by cascading small filters to capture large region information.\nTraining for deep CNNs is slow - VDSR addresses this by only learning residuals - the difference between the Low Resolution (LR) and High Resolution (HR) images. This works because the LR and HR images share the same information to a very large extent. Additionally, very large learning rates are used during training, with adjustable gradient clipping.\nSRCNN only works for a single scale - A single VDSR model is adequate for multi-scale-factor super resolution.\nProposed Method Proposed Network The network takes in an interpolated LR (ILR) image of shape $w \\times h \\times 3$ and predicts the residual image ($ w \\times h \\times 1$) which is then added onto the ILR to yield the HR image ($w \\times h \\times 3$).\nThe network is comprised of $L$ layers where all but $l=1,20$ (first and last) follow ZEROPAD -\u0026gt; CONV($3\\times 3, 64 \\text{ filters}$) -\u0026gt; RELU. The first layer operates on the input and the last layer consists of ZEROPAD -\u0026gt; CONV($3\\times 3, 1 \\text{ filter}$) to output the desired residual image.\nThe purpose of zero-padding before each convolution is to preserve the size of the feature maps. One issue with deep CNNs is that the convolution operation reduces the size of the feature map. Pixels on the border cannot be inferred properly, so usually SISR methods crop the boundary out which is fine for shallow models, but for deep CNNs it is unfeasible. Zero-padding addresses this issue, and is reported to work well.\n$L$ is specified to be $20$ in the paper\u0026rsquo;s training description.\nTraining The Loss function was the mean squared error averaged over the training set: $\\frac{1}{2} || \\pmb{y} - \\pmb{\\hat{y}}||^2$, where $\\pmb{y}$ is the HR image corresponding to the input LR image, and $\\pmb{\\hat{y}}$ is the model predicted HR image.\nResidual Learning The residual image is defined as $\\pmb{r}=\\pmb{y}-\\pmb{x}$. Most values are likely to be small or zero, which is desirable when training. Since we want the network to predict the residual $\\pmb{r}$, the loss function can be rewritten as $\\frac{1}{2} || \\pmb{r} - \\pmb{\\hat{y}}||^2$. However, in the actual network training, the loss is the $L_2$ norm betweeen the reconstructed image $\\pmb{r}+\\pmb{x}$ and the ground truth $\\pmb{y}$.\nMini-batch Gradient Descent was used with a momentum optimizer (I assume, as the paper references momentum $\\beta = 0.9$, could also be the Adam optimizer) and a weight decay of $0.0001$ (weight decay means adding a regularizing term to the loss, $\\mathcal{L} = \\frac{1}{2} || \\pmb{y} - \\pmb{\\hat{y}}||^2 + \\gamma L_2, \\gamma=0.0001$)\nAdjustable Gradient Clipping An issue when training deep CNNs is the slow speed of convergence. One tactic to speed up training is to increase the learning rate $\\alpha$, however this can lead to exploding gradients.\nOne solution to this is referred to as Gradient Clipping where the gradients of the parameters with respect to the loss function are clipped between a certain range $[-\\theta, \\theta]$. The issue with this approach is that, at the start of training when the learning rate is very high, $\\theta$ must be very small to prevent exploding gradients, however as the network is trained, learning rate is annealed and as such $\\alpha \\frac{\\partial{\\mathcal{L}}}{\\partial{W}}$ gets increasingly smaller.\nThe suggested method is to set gradients between $[-\\frac{\\theta}{\\alpha}, \\frac{\\theta}{\\alpha}]$, so the clipping is adjusted based on the current learning rate.\nMulti-Scale The model can be adapted to handle mutliple scales by simply training it on data of varying scales. Images are divided into sub-images without overlap where sub-images from different scales are present.\nThe paper tests the performance of a model trained with $s_{train}=\\{2\\}$ (scale factor of 2 in the training set) on different input scales and sees that for $s_{train} \\ne s_{test}$, performance is bad. However when $s_{train}=\\{2,3,4\\}$ the performance at each scale factor is comparable with a corresponding single-scale network, even outperforming single-scale models at large scales (3,4).\nResults VDSR outperforms Bicubic, A+, RFL, SelfEx, and SRCNN (all methods listed) in every regard (PSNR/SSIM/time).\nBenchmarks were made on Set5, Set14, B100 and Urban100 datasets.\nImplementation After reading the paper, I decided to implement VDSR in Keras. Please note this is a very quick-and-dirty implementation, it forgoes the adjustable gradient clipping and the learning rate adjustments made in the paper.\nI also test the model on one of the classes in the CIFAR10 dataset, namely the frog class.\nimport numpy as np\rimport matplotlib.pyplot as plt\rimport cv2\rimport tensorflow as tf\rfrom tensorflow.keras.models import Model\rfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, ReLU, Add, Input\rfrom tensorflow.keras.optimizers import Adam\rfrom tensorflow.keras.datasets import cifar10\rfrom tensorflow.keras.backend import resize_images\rdef vdsr(input_dim, l):\r#Define input layer\rLR = Input(shape=input_dim, name='input')\r#First convolution\rX = ZeroPadding2D()(LR)\rX = Conv2D(64,(3,3), name='CONV1')(X)\rX = ReLU()(X)\r#Repeat convolution layers untill last layer\rfor i in range(l-2):\rX = ZeroPadding2D()(X)\rX = Conv2D(64, (3,3), name='CONV%i' % (i+2))(X)\rX = ReLU()(X)\r#Final layer, output is residual image\rX = ZeroPadding2D()(X)\rresidual = Conv2D(1, (3,3), name='CONV%i' % l)(X)\r#Add residual to LR\rout = Add()([LR, residual])\rreturn Model(LR, out)\r#Load the cifar10 dataset\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\rclasses = ['airplane','automobile','bird','cat','deer','dog','frog', 'horse','ship','truck']\r#Example image\ri = np.random.randint(x_train.shape[0])\rplt.imshow(x_train[i])\rplt.axis('off')\rplt.title(classes[y_train[i][0]], color='w')\rplt.show()\r#I'll use just the frog classes\rtrain_idx = (y_train == [classes.index('frog')])\ry_train = x_train[np.squeeze(train_idx)]\rtest_idx = (y_test == [classes.index('frog')])\ry_test = x_test[np.squeeze(test_idx)]\rprint(\u0026quot;Training set ground truth has shape: \u0026quot; + str(y_train.shape))\rprint(\u0026quot;Test set ground truth has shape: \u0026quot; + str(y_test.shape))\rTraining set ground truth has shape: (5000, 32, 32, 3)\rTest set ground truth has shape: (1000, 32, 32, 3)\rw , h = y_train.shape[1:-1]\rscale_factor = 2\rx_train = []\rfor img in y_train:\rimg_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)\rx_train.append(img_re)\rx_train = np.asarray(x_train)\rx_test = []\rfor img in y_test:\rimg_re = cv2.resize(img, dsize=(w/scale_factor, h/scale_factor), interpolation=cv2.INTER_CUBIC)\r#Normalize\rx_test.append(img_re)\rx_test = np.asarray(x_test)\rprint(\u0026quot;Training set has shape: \u0026quot; + str(x_train.shape))\rprint(\u0026quot;Test set has shape: \u0026quot; + str(x_test.shape))\rTraining set has shape: (5000, 16, 16, 3)\rTest set has shape: (1000, 16, 16, 3)\r#Example sample and label\ri = np.random.randint(x_train.shape[0])\rfig, ax = plt.subplots(1,2)\rfig.set_size_inches(12,6)\rfig.set_facecolor('w')\rax[0].imshow(x_train[i])\rax[0].set_title('X', fontsize=14)\rax[0].axis('off')\rax[1].imshow(y_train[i])\rax[1].set_title('Y', fontsize=14)\rax[1].axis('off')\rplt.axis('off')\rplt.show()\r#Define the model\rinput_dim = y_train.shape[1:]\rL = 20\rmodel = vdsr(input_dim, L)\rmodel.compile(optimizer=Adam(learning_rate=0.000075,beta_1=0.9), loss='mse', metrics=['accuracy'])\r#Need to upscale input\rx_train_scaled = resize_images(x_train/255.0, scale_factor, scale_factor,'channels_last', interpolation='bilinear')\rx_test_scaled = resize_images(x_test/255.0, scale_factor, scale_factor,'channels_last', interpolation='bilinear')\rhistory = model.fit(x_train_scaled, y_train/255., batch_size=64, epochs=10, validation_data=(x_test_scaled, y_test/255.0))\rTrain on 5000 samples, validate on 1000 samples\rEpoch 1/10\r5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0030 - accuracy: 0.9441 - val_loss: 0.0028 - val_accuracy: 0.9451\rEpoch 2/10\r5000/5000 [==============================] - 138s 28ms/sample - loss: 0.0027 - accuracy: 0.9441 - val_loss: 0.0027 - val_accuracy: 0.9451\rEpoch 3/10\r5000/5000 [==============================] - 139s 28ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451\rEpoch 4/10\r5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0026 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451\rEpoch 5/10\r5000/5000 [==============================] - 135s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0026 - val_accuracy: 0.9451\rEpoch 6/10\r5000/5000 [==============================] - 142s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451\rEpoch 7/10\r5000/5000 [==============================] - 132s 26ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451\rEpoch 8/10\r5000/5000 [==============================] - 140s 28ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451\rEpoch 9/10\r5000/5000 [==============================] - 133s 27ms/sample - loss: 0.0025 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451\rEpoch 10/10\r5000/5000 [==============================] - 130s 26ms/sample - loss: 0.0024 - accuracy: 0.9441 - val_loss: 0.0025 - val_accuracy: 0.9451\rfig, ax = plt.subplots()\rfig.set_size_inches(12,12)\rfig.set_facecolor('w')\rax.plot(history.history['loss'], label='loss')\rax.plot(history.history['val_loss'], label='val_loss')\rplt.legend()\rplt.show()\r#Example sample and label\ri = np.random.randint(x_train.shape[0])\rx_i = resize_images(np.expand_dims(x_train[i],axis=0), scale_factor, scale_factor,'channels_last', interpolation='bilinear')\ry_hat = model.predict(x_i/255.0)[0]\rfig, ax = plt.subplots(1,3)\rfig.set_size_inches(12,6)\rfig.set_facecolor('w')\rax[0].imshow(x_train[i])\rax[0].set_title('X', fontsize=14)\rax[0].axis('off')\rax[1].imshow(y_train[i])\rax[1].set_title('Y', fontsize=14)\rax[1].axis('off')\rax[2].imshow(y_hat)\rax[2].set_title('Y_hat', fontsize=14)\rax[2].axis('off')\rplt.axis('off')\rplt.show()\rThe model seems to work well and converges quickly, even for a fixed learning rate. Of note is that the CIFAR10 images are of size $32\\times 32 \\times 3$, so to test super resolution with a scale factor of 2, the input images are resized from $16 \\times 16 \\times 3$. Compared to Set5\u0026rsquo;s $256 \\times 256 \\times 3$ images used in the paper, this test isn\u0026rsquo;t reflective of VDSR\u0026rsquo;s ability to infer detail.\n","date":1593193032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593193032,"objectID":"1c4c42a6b235d74c57aee4a741580d76","permalink":"https://Fquico1999.github.io/post/vdsr_paper/","publishdate":"2020-06-26T10:37:12-07:00","relpermalink":"/post/vdsr_paper/","section":"post","summary":"Using a Deep CNN to achieve highly accurate single-image super-resolution","tags":[],"title":"Accurate Image Super-Resolution Using Very Deep Convolutional Networks","type":"post"},{"authors":[],"categories":[],"content":"You can find this repository here\nOverview In an attempt to test and further my understanding of the mathematics and logistics behind neural networks and how they operate, I decided to follow what I learned in deeplearning.ai\u0026rsquo;s Neural Networks and Deep Learning course and implement Neural Networks from scratch using only NumPy.\nOutline To build a neural net from scratch, we need to go over each block and code those individually. At the end we can combine all of these to create an $L$-layer NN.\nSo, the steps we need to take are:\nParameter Intialization: We need to initialize parameters $W$ and $b$\rCompute a forward propagation pass: This involves computing the linear pass - $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ - and the activation $A^{[l]}=g(Z^{[l]})$ for both Sigmoid and ReLU activations\rCompute the loss\rImplement a back propagation pass\rUpdate the parameters: Here I'll code in mini Batch Gradient Descent (Which will cover both Stochastic Gradient Descent as well as Batch Gradient Descent), Momentum, RMSProp, and the king of them all, Adam\rimport math\rimport numpy as np\rimport matplotlib.pyplot as plt\rActivation Functions To add non-linearity to the model, activation functions are used. I\u0026rsquo;ll define them now. I\u0026rsquo;ll be using ReLU (rectified linear unit) and sigmoid in an example, but I\u0026rsquo;ll also define tanh and leaky ReLU.\ndef relu(Z):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rZ -- output of linear function Z = W*A+b\rReturns:\rret -- ReLU(Z)\rZ -- input for use in backprop\r\u0026quot;\u0026quot;\u0026quot;\rreturn np.maximum(0,Z), Z\rdef sigmoid(Z):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rZ -- output of linear function Z = W*A+b\rReturns:\rret -- sigmoid(Z)\rZ -- input for use in backprop\r\u0026quot;\u0026quot;\u0026quot;\rreturn 1./(1.+np.exp(-Z)), Z\rdef tanh(Z):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rZ -- output of linear function Z = W*A+b\rReturns:\rret -- tanh(Z)\rZ -- input for use in backprop\r\u0026quot;\u0026quot;\u0026quot;\rreturn np.tanh(Z), Z\rdef leaky_relu(Z):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rZ -- output of linear function Z = W*A+b\rReturns:\rret -- leaky_relu(Z)\rZ -- input for use in backprop\r\u0026quot;\u0026quot;\u0026quot;\rreturn np.maximum(0.01*Z, Z), Z\rParameter Initialization For passing parameter information between different functions, I\u0026rsquo;ll use a dictionary parameters, which will store $W$ and $b$ values for each layer $l {l:{0\\le l \\le L}}$\nAdditionally, I\u0026rsquo;ll implement random, Xavier initialization, and He initialization.\nRandom Initialization: Samples values from a normal distribution, and multiplies by a small value to keep weights close to zero - regularization\rXavier Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{1}{\\text{previous layer dimension}}}$\rHe Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{2}{\\text{previous layer dimension}}}$\rdef initialize_parameters(model_shape, initialization_method='he'):\r\u0026quot;\u0026quot;\u0026quot;\rInitializes parameters W and b of a network of shape model_shape.\rArguments:\rmodel_shape -- list containing the dimensions of each network layer l\rReturns:\rparameters -- dictionary containing weight and bias parameters\r\u0026quot;\u0026quot;\u0026quot;\r#define dictionary\rparams = {}\r#Obtain L\rL = len(model_shape)\r#Check initialization_method\rif initialization_method == 'random':\rbeta = 0.01\rfor l in range(1,L):\rparams[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta\rparams[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1])\relif initialization_method == 'xavier':\rL = L-1\rfor l in range(1,L+1):\rbeta = np.sqrt(1./model_shape[l-1])\rparams[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta\rparams[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1])\relif initialization_method == 'he':\rL = L - 1\rfor l in range(1,L+1):\rbeta = np.sqrt(2./model_shape[l-1])\rparams[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta\rparams[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1])\relse:\rraise NameError(\u0026quot;%s is not a valid initalization method\u0026quot;%(initialization_method))\rreturn params\rForward Propagation Forward propagation refers to passing through the computation graph from left to right - forwards - and evaluating $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ for each sucessive $l$ starting with $l=1$, in which case $A^{[0]}=X$, in other words, the activation fed into the first layer is simply the inputs.\nTo accomplish this, I\u0026rsquo;ll create two functions. The first will evaluate the linear formula $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$, whereas the second will evaluate $A^{[l]} = g(Z^{[l]})$, which corresponds to evaluating the activation function.\nThen forward_prop implements both to complete a forward propagation pass.\nIn order to compute the backprop later onwards, I\u0026rsquo;ll need to store $A^{[l]}$,$W^{[l]}$, $b^{[l]}$ as well as $Z^{[l]}$ which I\u0026rsquo;ll do in linear cache and activation cache\nOne of the arguments of forward_prop is layer_activations, which is a list of the activations for each layer of the neural network.\ndef forward_linear(W,A,b):\r\u0026quot;\u0026quot;\u0026quot;\rLinear part of forward propagation\rArguments:\rW -- weight matrix\rA -- activations\rb -- bias matrix\rReturns:\rZ -- input to the layer's activation function\rlinear_cache -- tuple with A, W, b for efficient backprop\r\u0026quot;\u0026quot;\u0026quot;\rZ = np.dot(W,A)+b\rlinear_cache = (A,W,b)\rassert(Z.shape == (W.shape[0], A.shape[1]))\rreturn Z, linear_cache\rdef forward_activation(Z, activation):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rZ -- Output of linear function Z = WA_prev+b\ractivation -- String denoting activation function to use. One of [linear, sigmoid, relu, leaky_relu, tanh, softmax]\rReturns:\rA -- g(Z), where g() is the corresponding activation\ractivation_cache -- the input Z, which will be fed into backprop\r\u0026quot;\u0026quot;\u0026quot;\rif activation == 'linear':\rA, activation_cache = Z, Z\relif activation == 'sigmoid':\rA, activation_cache = sigmoid(Z)\relif activation == 'relu':\rA, activation_cache = relu(Z)\relif activation == 'leaky_relu':\rA, activation_cache = leaky_relu(Z)\relif activation == 'tanh':\rA, activation_cache = tanh(Z)\relse:\rraise NameError('%s is not a valid activation function' %(activation))\rreturn A, activation_cache\rdef forward_prop(X, layer_activations, parameters):\r\u0026quot;\u0026quot;\u0026quot;\rImplements one pass of forward propagation\rArguments:\rX -- input data\rlayer_activations -- list of strings corresponding to the activations of each layer\rparameters -- output of initialize_parameters\rReturns:\rA - Output of activation function of the last layer\rcaches - list of caches containing both linear and activation caches\r\u0026quot;\u0026quot;\u0026quot;\r#Define caches\rcaches = []\r#A[0] is the input\rA = X\rL = len(parameters)//2 for l in range(1, L+1):\rA_prev = A\rW = parameters[\u0026quot;W\u0026quot;+str(l)]\rb = parameters[\u0026quot;b\u0026quot;+str(l)]\rZ, linear_cache = forward_linear(W, A_prev, b)\rA, activation_cache = forward_activation(Z, layer_activations[l])\rassert (A.shape == (W.shape[0], A_prev.shape[1]))\r#Add both linear and activation cache to caches\rcaches.append((linear_cache, activation_cache))\rreturn A, caches\rCost Function The cost function is the metric that a neural net aims to minimize. I\u0026rsquo;ll implement cross-entropy cost, given by:\n$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\nThus, we require a method of computing cost after one pass of forward propagation.\ndef cost(A_last, Y):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rA_last -- Post-activation value of the last layer of the network\rY -- Groud truth vectors\rReturns:\rcost -- cross-entropy cost\r\u0026quot;\u0026quot;\u0026quot;\r#Get number of samples, m\rm = Y.shape[1]\r#Compute cross entropy cost\rcost = -(1.0/m)*np.sum(Y*np.log(A_last)+(1.-Y)*np.log(1.-A_last))\r#Ensure appropriate dimensions\rcost = np.squeeze(cost)\rreturn cost\rBack Propagation To update our parameters, we need to calculate the gradient of the loss with respect to $W$ and $b$\nJust like with forward prop, I will implement two functions. One deals with the back pass for the linear part of the units and the other deals with the derivatives of the activation functions.\nFor the linear part, we take the derivatives of the parameters, obtaining:\n$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$ $$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$ $$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} $$\nFor the activation part, the backprop requires the gradient of the activation function. As such it depends on the activation used, and I\u0026rsquo;ll define them for each one.\nFor sigmoid:\n$$ \\sigma{(z)} = \\frac{1}{1+e^{-x}}$$ $$\\frac{d\\sigma{(z)}}{dz} = \\sigma{(z)}(1-\\sigma{(z)})$$\nFor ReLU:\n$$\\text{ReLU}(z) = \\max{(0,z)}$$ $$\\frac{d\\text{ReLU}}{dz} = \\left\\{\\begin{array}{ll}1 , z \u0026gt; 0\\\\0, z \\le 0\\end{array}\\right.$$\nNote that for ReLU, strictly speaking, there is a discontinuity at $z=0$, however since it is incredibly unlikely that the input to the function will every be exactly zero, it\u0026rsquo;s fine to include it in $z\\le0$\nFor tanh: $$\\tanh{(z)} = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$ $$\\frac{d\\tanh(z)}{dz} = 1-\\tanh^2(z)$$\nFor leaky ReLU: $$\\text{leaky ReLU}(z) = \\max(0.01z, z)$$ $$\\frac{d(\\text{leaky Relu}(z))}{dz} = \\left\\{\\begin{array}{ll}1 , z \u0026gt; 0\\\\0.01, z \\le0\\end{array}\\right.$$\nSo, I\u0026rsquo;ll implement functions for each of these units to compute: $$dZ^{[l]} = dA^{[l]} * g\u0026rsquo;(Z^{[l]})$$\nAdditionally, to initialize backpropagation, we need $\\frac{d\\mathcal{L}}{dA^{[L]}}$, the gradient of the cost function with respect to the last activation output. For cross-entropy this is: $$-\\sum\\limits_{i=1}^{m}\\frac{y^{i}}{a^{[L](i)}} - \\frac{1-y^{i}}{1-a^{[L](i)}}$$\ndef backward_linear(dZ, cache):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rdZ -- Gradient of cost w.r.t linear portion\rcache -- tuple coming from cached forward prop of layer l\rReturns:\rdA_prev -- gradient with respect to activation of previous layer\rdW -- gradient with respect to weights of current layer\rdb -- gradient with respect to biases of current layer\r\u0026quot;\u0026quot;\u0026quot;\r#unpack cache\rA_prev, W, b = cache\r#Get number of samples\rm = A_prev.shape[1]\rdW = 1./m*np.dot(dZ, A_prev.T)\rdb = 1./m*np.sum(dZ, axis=1, keepdims=True)\rdA_prev = np.dot(W.T, dZ)\rassert (dA_prev.shape == A_prev.shape)\rassert (dW.shape == W.shape)\rassert (db.shape == b.shape)\rreturn dA_prev, dW, db\rdef backward_activation(dA, Z, activation):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rdA -- post-activation gradient for current layer l Z -- cached matrix from forward prop\ractivation -- the activation to be used in the layer\rReturns:\rdZ -- gradient of cost function with respect to Z[l]\r\u0026quot;\u0026quot;\u0026quot;\rif activation == 'linear':\rdZ = dA\relif activation == \u0026quot;relu\u0026quot;:\rdZ = np.array(dA, copy=True)\rdZ[Z \u0026lt;= 0] = 0\relif activation == \u0026quot;sigmoid\u0026quot;:\rs = 1./(1+np.exp(-Z))\rdZ = dA * s * (1-s)\relif activation == \u0026quot;leaky_relu\u0026quot;:\rdZ = np.array(dA, copy=True)\rdZ[Z \u0026lt;= 0] = 0.01\relif activation == \u0026quot;tanh\u0026quot;:\rdZ = dA*(1 - tanh(Z)**2)\relse:\rraise NameError(\u0026quot;%s is not a valid activation function\u0026quot; % (activation))\rassert(dZ.shape == Z.shape)\rreturn dZ\rdef backward_prop(AL, Y, caches, layer_activations):\r\u0026quot;\u0026quot;\u0026quot;\rImplement a backward propagation pass\rArguments:\rAL -- output of the forward propagation\rY -- ground truth\rcaches -- list of caches containing linear_cache and activation_cache\rReturns:\rgrads -- A dictionary with the gradients dA[l], dW[l], db[l]\r\u0026quot;\u0026quot;\u0026quot;\r#Define dict to store gradients for parameter update\rgrads = {}\rL = len(caches)\rm = AL.shape[1]\r#Ensure Y is the same as AL (which is essentially y_hat)\rY = Y.reshape(AL.shape)\r#Initialize backprop, a.k.a derivative of cost with respect to AL\rdAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\rgrads[\u0026quot;dA\u0026quot;+str(L)] = dAL\rfor l in reversed(range(L)):\rcurrent_cache = caches[l]\rlinear_cache, activation_cache = current_cache\rdZ = backward_activation(grads[\u0026quot;dA\u0026quot;+str(l+1)],activation_cache, layer_activations[l])\rdA_prev, dW, db = backward_linear(dZ, linear_cache)\rgrads[\u0026quot;dA\u0026quot; + str(l)] = dA_prev\rgrads[\u0026quot;dW\u0026quot; + str(l + 1)] = dW\rgrads[\u0026quot;db\u0026quot; + str(l + 1)] = db\rreturn grads\rUpdate Parameters The final step is to take the gradients computed in back propagation and use them to update the parameters $W$ and $b$.\nThe method of updating these parameters is important and there are several optimizers that do this in different ways.\nMini-Batch Gradient Descent: $$ W:=W-\\alpha dW $$ $$ b:=b-\\alpha db $$ For the other optimization algorithms, the concept of exponentially weighted averages becomes an important one. An exponentially weighted average can be calculated with the following formula: $$v_{\\theta, i} := \\beta v_{\\theta, i} + (1-\\beta)\\theta_{i}$$\nWhere $\\theta_{i}$ are the samples in the dataset to average over. The parameter $\\beta$ roughly controls how many samples to average over given by approximately $\\frac{1}{1-\\beta}$. Most commonly in momentum, $\\beta=0.9$, which works out to averaging over the last 10 samples.\nMomentum: $$ \\begin{cases} v_{dW^{[l]}} := \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\ W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}} \\end{cases}$$ $$\\begin{cases} v_{db^{[l]}} := \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\ b^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}} \\end{cases}$$\nRMSProp: $$ \\begin{cases} s_{dW^{[l]}} := \\beta s_{dW^{[l]}} + (1 - \\beta) (dW^{[l]})^{2} \\\\ W^{[l]} := W^{[l]} - \\alpha \\frac{dW^{[l]}}{\\sqrt{s_{dW^{[l]}}}+\\epsilon} \\end{cases}$$ $$\\begin{cases} s_{db^{[l]}} := \\beta s_{db^{[l]}} + (1 - \\beta) (db^{[l]})^{2} \\\\ b^{[l]} := b^{[l]} - \\alpha \\frac{db^{[l]}}{\\sqrt{s_{db^{[l]}}}+\\epsilon} \\end{cases}$$\nNote the addition of $\\epsilon$ in the denominator in both RMSProp and Adam. That is to prevent NaNs or divisions by zero, it increases numerical stability. The king of the optimizers, Adam, works by combining both momentum and RMSProp. Additionally, it also adds bias correction to the exponentially weighted averages $v$ and $s$. The need for bias correction comes from the fact that as the number of samples that we average over increases, the beginning of the averaging causes the output to be very small since at the start we only have one sample and the others are initialized to zero. As such, the start of our averaging results in a much lower start than the original distribution.\nAdam: $$\\begin{cases} v_{dW^{[l]}} := \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) dW^{[l]} \\\\ v^{corrected}{dW^{[l]}} = \\frac{v{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\ s_{dW^{[l]}} := \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (dW^{[l]})^2 \\\\ s^{corrected}{dW^{[l]}} = \\frac{s{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\ W^{[l]} := W^{[l]} - \\alpha \\frac{v^{corrected}{dW^{[l]}}}{\\sqrt{s^{corrected}{dW^{[l]}}} + \\varepsilon} \\end{cases}$$ $$\\begin{cases} v_{db^{[l]}} := \\beta_1 v_{db^{[l]}} + (1 - \\beta_1) db^{[l]} \\\\ v^{corrected}{db^{[l]}} = \\frac{v{db^{[l]}}}{1 - (\\beta_1)^t} \\\\ s_{db^{[l]}} := \\beta_2 s_{db^{[l]}} + (1 - \\beta_2) (db^{[l]})^2 \\\\ s^{corrected}{db^{[l]}} = \\frac{s{db^{[l]}}}{1 - (\\beta_2)^t} \\\\ b^{[l]} := b^{[l]} - \\alpha \\frac{v^{corrected}{db^{[l]}}}{\\sqrt{s^{corrected}{db^{[l]}}} + \\varepsilon} \\end{cases}$$\nThe $t$ parameter in Adam included in the bias correction formula is the number of steps taken.\nBesides functions to update these parameters, we also need functions to initialize them (except for gradient descent)\n## Gradient Descent\rdef update_parameters_gd(parameters, grads, learning_rate=0.01):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- parameters W and b\rgrads -- gradients from backprop - dW and db\rReturns:\rparameters -- parameters W and b updated using gradient descent update rules\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2 # number of layers\rfor l in range(L):\rparameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*grads[\u0026quot;dW\u0026quot;+str(l+1)]\rparameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)]- learning_rate*grads[\u0026quot;db\u0026quot;+str(l+1)]\rreturn parameters\r## Momentum\rdef initialize_parameters_momentum(parameters):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- dictionary containing parameters W,b\rReturns:\rvelocities -- initialized velocities for momentum updates\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2\rvelocities = {}\r# Initialize velocities\rfor l in range(L):\rvelocities[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\rvelocities[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\rreturn velocities\rdef update_parameters_momentum(parameters, grads, velocities, learning_rate=0.01, beta=0.9):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- parameters W and b\rgrads -- gradients from backprop - dW and db\rvelocities -- current velocities for momentum\rReturns:\rparameters -- parameters W and b updated using momentum update rules\rvelocities -- updated velocities\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2\rfor l in range(L):\r# compute velocities using exponential weighted average\rvelocities[\u0026quot;dW\u0026quot; + str(l+1)] = beta*velocities[\u0026quot;dW\u0026quot;+str(l+1)]+(1-beta)*grads[\u0026quot;dW\u0026quot;+str(l+1)]\rvelocities[\u0026quot;db\u0026quot; + str(l+1)] = beta*velocities[\u0026quot;db\u0026quot;+str(l+1)]+(1-beta)*grads[\u0026quot;db\u0026quot;+str(l+1)]\r#parameter update\rparameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*velocities[\u0026quot;dW\u0026quot; + str(l+1)]\rparameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*velocities[\u0026quot;db\u0026quot; + str(l+1)]\rreturn parameters, velocities\r## RMSProp\rdef initialize_parameters_rmsprop(parameters):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- dictionary containing parameters W,b\rReturns: squares -- initialized moving average of the squared gradient for rmsprop updates\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2 squares = {}\r# Initialize squares\rfor l in range(L):\rsquares[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\rsquares[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\rreturn squares\rdef update_parameters_rmsprop(parameters, grads, squares, learning_rate=0.01,\rbeta=0.9, epsilon=1e-8):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- parameters W and b\rgrads -- gradients from backprop - dW and db\rsquares -- current squres of past gradients for rmsprop\rReturns:\rparameters -- parameters W and b updated using rmsprop update rules\rsquares -- updated squares\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2\rfor l in range(L):\r# compute velocities using exponential weighted average\rsquares[\u0026quot;dW\u0026quot; + str(l+1)] = beta*squares[\u0026quot;dW\u0026quot;+str(l+1)]+(1-beta)*(grads[\u0026quot;dW\u0026quot;+str(l+1)]**2)\rsquares[\u0026quot;db\u0026quot; + str(l+1)] = beta*squares[\u0026quot;db\u0026quot;+str(l+1)]+(1-beta)*(grads[\u0026quot;db\u0026quot;+str(l+1)]**2)\r#parameter update\rparameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*(grads[\u0026quot;dW\u0026quot;+str(l+1)]/(np.sqrt(squares[\u0026quot;dW\u0026quot; + str(l+1)])+epsilon))\rparameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*(grads[\u0026quot;db\u0026quot;+str(l+1)]/(np.sqrt(squares[\u0026quot;db\u0026quot; + str(l+1)])+epsilon))\rreturn parameters, squares\r## Adam\rdef initialize_parameters_adam(parameters):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- dictionary containing parameters W,b\rReturns: velocities -- initialized first gradient weighted averages for adam updates\rsquares -- initialized moving average of the squared gradient for adam updates\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2 velocities = {}\rsquares = {}\r# Initialize velocities and squares\rfor l in range(L):\rvelocities[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\rvelocities[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\rsquares[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\rsquares[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\rreturn velocities, squares\rdef update_parameters_adam(parameters, grads, velocities, squares, t, learning_rate=0.01,\rbeta1=0.9, beta2=0.999, epsilon=1e-8):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rparameters -- dictionary with parameters W, b\rgrads -- dictionary with gradients dW, db\rvelocities -- moving average of the first gradient\rsquares -- moving average of the squared gradient\rt -- counter for bias correction\rReturns:\rparameters -- updated parameters according to adam\rvelocities -- updated moving average of the first gradient\rsquares -- updated moving average of the squared gradient\r\u0026quot;\u0026quot;\u0026quot;\rL = len(parameters) // 2 v_corrected = {} s_corrected = {} for l in range(L):\r#Calculate exponentially weighted velocities\rvelocities[\u0026quot;dW\u0026quot; + str(l+1)] = beta1*velocities[\u0026quot;dW\u0026quot; + str(l+1)]+(1-beta1)*grads[\u0026quot;dW\u0026quot; + str(l+1)]\rvelocities[\u0026quot;db\u0026quot; + str(l+1)] = beta1*velocities[\u0026quot;db\u0026quot; + str(l+1)]+(1-beta1)*grads[\u0026quot;db\u0026quot; + str(l+1)]\r#Bias correction for velocities\rv_corrected[\u0026quot;dW\u0026quot; + str(l+1)] = velocities[\u0026quot;dW\u0026quot; + str(l+1)]/(1-beta1**t)\rv_corrected[\u0026quot;db\u0026quot; + str(l+1)] = velocities[\u0026quot;db\u0026quot; + str(l+1)]/(1-beta1**t)\r#Calculate exponentially weighted squares\rsquares[\u0026quot;dW\u0026quot; + str(l+1)] = beta2*squares[\u0026quot;dW\u0026quot; + str(l+1)]+(1-beta2)*grads[\u0026quot;dW\u0026quot; + str(l+1)]**2\rsquares[\u0026quot;db\u0026quot; + str(l+1)] = beta2*squares[\u0026quot;db\u0026quot; + str(l+1)]+(1-beta2)*grads[\u0026quot;db\u0026quot; + str(l+1)]**2\r#Bias correction for squares\rs_corrected[\u0026quot;dW\u0026quot; + str(l+1)] = squares[\u0026quot;dW\u0026quot; + str(l+1)]/(1-beta2**t)\rs_corrected[\u0026quot;db\u0026quot; + str(l+1)] = squares[\u0026quot;db\u0026quot; + str(l+1)]/(1-beta2**t)\r#Adam parameter updates\rparameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*(v_corrected[\u0026quot;dW\u0026quot; + str(l+1)]/(np.sqrt(s_corrected[\u0026quot;dW\u0026quot; + str(l+1)])+epsilon))\rparameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*(v_corrected[\u0026quot;db\u0026quot; + str(l+1)]/(np.sqrt(s_corrected[\u0026quot;db\u0026quot; + str(l+1)])+epsilon))\rreturn parameters, velocities, squares\rCombining Everything and Mini-Batch GD After going through each piece, we now need to combine all these functions to train a model. To do this, we have some input data $X$ with respective labels $Y$. Now, to implement mini-bach gradient descent, we need to split $X$ and $Y$ into $m$ mini-batches to run our algorithms on.\ndef mini_batches(X, Y, mini_batch_size = 64, seed = 0):\r\u0026quot;\u0026quot;\u0026quot;\rArguments:\rX -- input data\rY -- corresponding labels\rmini_batch_size -- size of the mini-batches\rseed -- used to set np.random.seed differently to get different shuffles\rReturns:\rmini_batches -- list of (mini_batch_X, mini_batch_Y)\r\u0026quot;\u0026quot;\u0026quot;\r#Set seed\rnp.random.seed(seed)\rmini_batches = []\r#Get number of examples\rm = X.shape[1] idx = list(np.random.permutation(m))\rshuffled_X = X[:, idx]\rshuffled_Y = Y[idx, :]\rshuffled_Y = np.reshape(shuffled_Y,(1,m))\rassert shuffled_Y.shape == (1,m)\r#Need to account for when minibatch size is divisible by m num_full_minibatch = int(math.floor(m/mini_batch_size))\rfor i in range(0, num_full_minibatch):\rmini_batch_X = shuffled_X[:,mini_batch_size*i: mini_batch_size*(i+1)]\rmini_batch_Y = shuffled_Y[:,mini_batch_size*i: mini_batch_size*(i+1)]\rmini_batches.append((mini_batch_X, mini_batch_Y))\r# Now need to take care of extra examples of len \u0026lt; m\rif m % mini_batch_size != 0:\rmini_batch_X = shuffled_X[:,-(mini_batch_size-m):]\rmini_batch_Y = shuffled_Y[:,-(mini_batch_size-m):]\rmini_batches.append((mini_batch_X, mini_batch_Y))\rreturn mini_batches\rdef train(X, Y, model_shape, layer_activations, optimizer, initialization_method='he', learning_rate = 0.001, mini_batch_size = 64, beta = 0.9,\rbeta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True):\r\u0026quot;\u0026quot;\u0026quot;\rImplementation of a Neural Network model.\rArguments:\rX -- input data\rY -- labels\rmodel_shape -- python list with the size of each layer\rlayer_activations -- python list with activation of each layer\roptimizer -- string corresponding to optimizer to use. One of \u0026quot;gd\u0026quot;,\u0026quot;momentum\u0026quot;,\u0026quot;rmsprop\u0026quot;,\u0026quot;adam\u0026quot;\rlearning_rate -- the learning rate parameter\rmini_batch_size -- the size of each mini batch\rbeta -- Momentum/RMSProp hyperparameter\rbeta1 -- decay of past gradients parameter for adam\rbeta2 -- decay of past squared gradients for adam\repsilon -- hyperparameter preventing division by zero in Adam and RMSProp updates\rnum_epochs -- number of epochs\rprint_cost -- True to print the cost every 5 epochs\rReturns:\rparameters -- trained parameters\r\u0026quot;\u0026quot;\u0026quot;\r#Track costs\rcosts = [] #Adam bias correction parameter\rt = 0 #define seed for np.random.seed in mini_batch call\rseed = np.random.randint(1000)\r#Number of layers and number of training examples\rL = len(model_shape)\rm = X.shape[1]\r# Initialize parameters\rparameters = initialize_parameters(model_shape, initialization_method=initialization_method)\r# Initialize parameters for optimizer\rif optimizer == \u0026quot;gd\u0026quot;:\rpass\relif optimizer == \u0026quot;momentum\u0026quot;:\rvelocities = initialize_parameters_momentum(parameters)\relif optimizer == 'rmsprop':\rsquares = initialize_parameters_rmsprop(parameters)\relif optimizer == \u0026quot;adam\u0026quot;:\rvelocities, squares = initialize_parameters_adam(parameters)\relse:\rraise NameError(\u0026quot;%s is not a valid optimizer\u0026quot; % (optimizer))\r#Loop\rfor i in range(num_epochs):\r# Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\rseed = seed + 1\rminibatches = mini_batches(X, Y, mini_batch_size, seed)\r#Get cost over all batchs\rtotal_cost = 0\rfor minibatch in minibatches:\r# Unpack\r(minibatch_X, minibatch_Y) = minibatch\r# Forward propagation pass\rAL, caches = forward_prop(minibatch_X, layer_activations, parameters)\r#Get minibatch cost\rcost_batch = cost(AL, minibatch_Y)\r#Add to total cost\rtotal_cost+=cost_batch\r# Backward propagation pass\rgrads = backward_prop(AL, minibatch_Y, caches, layer_activations)\r# Update parameters\rif optimizer == \u0026quot;gd\u0026quot;:\rparameters = update_parameters_gd(parameters, grads, learning_rate=learning_rate)\relif optimizer == \u0026quot;momentum\u0026quot;:\rparameters, velocities = update_parameters_momentum(parameters,grads,\rvelocities,learning_rate=learning_rate,\rbeta=beta)\relif optimizer == \u0026quot;rmsprop\u0026quot;:\rparameters, squares = update_parameters_rmsprop(parameters, grads, squares,\rlearning_rate=learning_rate, beta=beta,\repsilon=epsilon)\relif optimizer == \u0026quot;adam\u0026quot;:\r#Increment bias correction parameter\rt = t + 1\rparameters, velocities, squares = update_parameters_adam(parameters, grads,\rvelocities, squares,\rt, learning_rate=learning_rate,\rbeta1=beta1, beta2=beta2,\repsilon=epsilon)\rmean_cost = total_cost / float(mini_batch_size)\r# Print the cost every 5 epoch\rif print_cost and i % 5 == 0:\rprint (\u0026quot;Cost after epoch %i: %f\u0026quot; %(i, mean_cost))\rif print_cost and i % 1 == 0:\rcosts.append(mean_cost)\r# plot the cost\rfig, ax = plt.subplots()\rfig.set_facecolor('w')\rfig.set_size_inches(12,9)\rax.plot(costs)\rax.set_ylabel('Cost')\rax.set_xlabel('Epoch')\rplt.title(\u0026quot;Learning rate = %s, Optimizer = %s\u0026quot; % (learning_rate, optimizer))\rplt.show()\rreturn parameters\rTesting the Model Now that the implementation is complete, let\u0026rsquo;s test the model by doing binary classification on two handwritten digits from the MNIST dataset.\nfrom tensorflow.keras.datasets import mnist\r(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\rimg_shape = X_train.shape[1:]\rprint('X_train has shape %s\\nY_train has shape %s'%(X_train.shape, Y_train.shape))\rX_train has shape (60000, 28, 28)\rY_train has shape (60000,)\r#Convert Y_train and Y_test to (m,1)\rY_train = Y_train.reshape(Y_train.shape[0],1)\rY_test = Y_test.reshape(Y_test.shape[0],1)\r#Visualize one Entry\ri = np.random.randint(X_train.shape[0])\rfig,ax = plt.subplots()\rfig.set_facecolor('w')\rax.imshow(X_train[i])\rax.set_title('Label = ' + str(Y_train[i]))\rplt.show()\r#Choose two classes for our classification model\rclass_a = 3 #Positive Class\rclass_b = 7 #Negative Class\r#Filter out the dataset to include only images in those classes\ridx = np.logical_or(np.squeeze(Y_train) == class_a, np.squeeze(Y_train) == class_b)\rX_train, Y_train = X_train[idx], Y_train[idx]\r#Assign class_a = 1 and class_b=0\rY_train[np.where(Y_train == class_a)] = 1.00\rY_train[np.where(Y_train == class_b)] = 0.00\rprint('X_train has shape %s\\nY_train has shape %s'%(X_train.shape, Y_train.shape))\ridx = np.logical_or(np.squeeze(Y_test) == class_a, np.squeeze(Y_test) == class_b)\rX_test, Y_test = X_test[idx], Y_test[idx].astype(np.float64)\r#Assign class_a = 1 and class_b=0\rY_test[np.where(Y_test == class_a)] = 1.00\rY_test[np.where(Y_test == class_b)] = 0.00\rprint('X_test has shape %s\\nY_test has shape %s'%(X_test.shape, Y_test.shape))\rX_train has shape (12396, 28, 28)\rY_train has shape (12396, 1)\rX_test has shape (2038, 28, 28)\rY_test has shape (2038, 1)\r#Reshape X_train and X_test into (m, 28*28)\rX_train_flat = X_train.reshape(X_train.shape[0], -1).T X_test_flat = X_test.reshape(X_test.shape[0], -1).T\r# Standardize data to have feature values between 0 and 1.\rX_train_norm = X_train_flat/255.\rX_test_norm = X_test_flat/255.\rprint (\u0026quot;X_train's shape: \u0026quot; + str(X_train_norm.shape))\rprint (\u0026quot;X_test's shape: \u0026quot; + str(X_test_norm.shape))\rX_train's shape: (784, 12396)\rX_test's shape: (784, 2038)\rDefining our Model I\u0026rsquo;ve chosen to create a model to classify either a $3$ or a $7$. Now, let\u0026rsquo;s define a model.\nThe output is either $1$ or $0$, where $1$ corresponds to a $3$ and $0$ corresponds to a $7$. This means the last layer dimension needs to be $1$. For the first dimension, that should be $28\\times28\\times1=784$, since we\u0026rsquo;re taking the image and stacking each row of pixels ontop of each other (flattening). For our hidden layer, I\u0026rsquo;ll choose $n_h=7$ So we have a three layer model - $784\\times7\\times1$ with layer activations ReLU-ReLU-Sigmoid.\nWe can compare the performance of gradient descent versus adam optimization. Let\u0026rsquo;s start with gradient descent.\n#Model Parameters\rn_x = X_train_norm.shape[0]\rn_y = 1\rn_h = 7\rmodel_shape = (n_x, n_h, n_y)\rlayer_activations = ['relu','relu','sigmoid']\roptimizer = 'gd'\rlearning_rate = 0.0005\rparameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer,\rlearning_rate=learning_rate, mini_batch_size=128, num_epochs=17)\rCost after epoch 0: 0.458453\rCost after epoch 5: 0.259276\rCost after epoch 10: 0.162094\rCost after epoch 15: 0.085669\rEvaluating our Model Now that the model has trained, we need some way of assessing the performance of our model. This is done with our testing set: $(X_{\\text{test}}, Y_{\\text{test}})$ Essentially, we just need to feed $X_{\\text{test}}$ through our model\u0026rsquo;s forward pass, which outputs $A^{[L]}=\\hat{Y}$, our predictions. Then we simply compare $\\hat{Y}$ with $Y_{\\text{test}}$ and evaluate the accuracy as $A=\\frac{\\text{# correct}}{\\text{# total}}$. Additionally, I\u0026rsquo;ll return the indices where the model predicted correctly, and where it predicted incorrectly, to visualize the model\u0026rsquo;s shortcomings.\ndef evaluate(X_test, Y_test, layer_activations, parameters, threshold=0.5):\r\u0026quot;\u0026quot;\u0026quot;\rEvaluates performance of trained model on test set\rAttributes:\rX_test -- Test set inputs\rY_test -- Test set labels\rlayer_activations -- python list of strings corresponding to activation functions of layer l\rparameters -- trained parameters W, b\rReturns:\rcorrect -- list of booleans corresponding to the indices of correct predictions\rincorrect -- list of booleans correspondingin to the indices of incorrect predictions\r\u0026quot;\u0026quot;\u0026quot;\r#Number of test samples\rm = X_test.shape[1]\rassert Y_test.shape == (1,m)\rY_pred, _ = forward_prop(X_test, layer_activations, parameters)\r#Threshold\rY_pred[Y_pred\u0026gt;threshold]=1.\rY_pred[Y_pred\u0026lt;=threshold]=0\rnum_correct = np.sum(Y_pred == Y_test)\rnum_incorrect = m-num_correct\rprint(\u0026quot;Accuracy: %f\u0026quot; % (float(num_correct)/m))\rcorrect = Y_pred == Y_test\rincorrect = Y_pred != Y_test\rreturn np.squeeze(correct), np.squeeze(incorrect)\r#Evaluate\rcorrect, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters)\r#Get correect predictions\rX_correct = X_test[correct]\rY_correct = Y_test[correct]\r#Get incorrect predictions\rX_incorrect = X_test[incorrect]\rY_incorrect = Y_test[incorrect]\rfig,ax = plt.subplots(3,2)\rfig.set_size_inches(12,18)\rfig.set_facecolor('w')\ri_correct = np.random.randint(len(X_correct), size=3)\ri_incorrect = np.random.randint(len(X_incorrect), size=3)\rfor i in range(3):\rax[i,0].imshow(X_correct[i_correct[i]])\rax[i,0].set_title(\u0026quot;%i: Correctly predicted Y=%i\u0026quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b))\rax[i,1].imshow(X_incorrect[i_incorrect[i]])\rax[i,1].set_title(\u0026quot;%i: Incorrectly predicted Y=%i\u0026quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a))\rax[i,0].xaxis.set_visible(False)\rax[i,0].yaxis.set_visible(False)\rax[i,1].xaxis.set_visible(False)\rax[i,1].yaxis.set_visible(False)\rplt.show()\rAccuracy: 0.964671\rAdam Optimization Now that we\u0026rsquo;ve gotten results using Gradient Descent, Let\u0026rsquo;s compare it with adam optimization\n#Model Parameters\rn_x = X_train_norm.shape[0]\rn_y = 1\rn_h = 7\rmodel_shape = (n_x, n_h, n_y)\rlayer_activations = ['relu','relu','sigmoid']\roptimizer = 'adam'\rlearning_rate = 0.0005\rparameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer,\rlearning_rate=learning_rate, mini_batch_size=128, num_epochs=5)\rCost after epoch 0: 0.347253\r#Evaluate\rcorrect, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters)\r#Get correect predictions\rX_correct = X_test[correct]\rY_correct = Y_test[correct]\r#Get incorrect predictions\rX_incorrect = X_test[incorrect]\rY_incorrect = Y_test[incorrect]\rfig,ax = plt.subplots(3,2)\rfig.set_size_inches(12,18)\rfig.set_facecolor('w')\ri_correct = np.random.randint(len(X_correct), size=3)\ri_incorrect = np.random.randint(len(X_incorrect), size=3)\rfor i in range(3):\rax[i,0].imshow(X_correct[i_correct[i]])\rax[i,0].set_title(\u0026quot;%i: Correctly predicted Y=%i\u0026quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b))\rax[i,1].imshow(X_incorrect[i_incorrect[i]])\rax[i,1].set_title(\u0026quot;%i: Incorrectly predicted Y=%i\u0026quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a))\rax[i,0].xaxis.set_visible(False)\rax[i,0].yaxis.set_visible(False)\rax[i,1].xaxis.set_visible(False)\rax[i,1].yaxis.set_visible(False)\rplt.show()\rAccuracy: 0.969578\rComparison As we can see, using the adam optimizer yielded better accuracy in nearly one third of the number of epochs.\n","date":1591765774,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591765774,"objectID":"cb30a4de54be5496c51334908322df12","permalink":"https://Fquico1999.github.io/project/numpy_neural_nets/","publishdate":"2020-06-09T22:09:34-07:00","relpermalink":"/project/numpy_neural_nets/","section":"project","summary":"Implementation of Neural Networks using only NumPy","tags":["Deep Learning"],"title":"NumPy Neural Networks","type":"project"},{"authors":[],"categories":[],"content":"The results for the Robocup@Home Education 2020 Online Competition are out! Check out our standing below.\nClick here if you want to skip to my involvement in this team.\nOverview UBC Open Robotics is a student team comprised of 60 students split into three subteams - ArtBot, PianoBot, and Robocup@Home. I am a member of the software team in the RoboCup@Home subteam.\nThe objective of RoboCup@Home is to build a household assistant robot that can perform a variety of tasks, including carrying bags, introducing and seating guests at a party, answering a variety of trivia questions and more. Open Robotics is developing a robot to compete in the 2021 RoboCup@Home Education Challenge while in the meantime, our subteam will compete in the 2020 Competition using the Turtlebot 2 as our hardware platform.\nThe Challenge The rules for the 2020 Challenge can be found here, but they boil down to three specific tasks:\nCarry My Luggage - Navigation task Find My Mates - Vision task Receptionist - Speech task Carry My Luggage Goal: The robot helps the operator to carry a bag to the car parked outside\nStarting at a predifined location, the robot has to find the operator and pick up the bag the operator is pointing to. After picking up the bag, the robot needs to indicate that it is ready to follow and then it must follow the operator while facing 4 obstacles along the way (crowd, small object, difficult to see 3D object, small blocked area).\nFind My Mates Goal: The robot fetches the information of the party guests for the operator who knows only the names of the guests. Knowing only the operator, the robot must identify unknown people and meet those that are waving. Afterwards, it must remember the person and provide a unique description of that person, as well as that person\u0026rsquo;s location, to the operator.\nReceptionist Goal: The robot has to take two arriving guests to the living room, introducing them to each other, and offering the just-arrived guest an unoccupied place to sit. Knowing the host of the party, John, the robot must identify unknown guests, request their names and favourite drinks and then point to an empty seat where the guest can sit.\nMy Contributions My main contributions have been in speech recognition and in handle segmentation, targeting task 3 and task 1 respectively, however I also worked on facial recognition earlier in the project.\nSpeech Recognition You can find this repository here\nSpeech recognition is implemented using PocketSphinx which is based on CMUSphinx. Which offers two modes of operation - Keyword Spotting (KWS) and Language Model (LM).\nKWS Keyword spotting tries to detect specific keywords or phrases, without imposing any type of grammer rules ontop. Utilizing keyword spotting requires a .dic file and a .kwslist file.\nThe dictionary file is a basic text file that contains all the keywords and their phonetic pronunciation, for instance:\nBACK\tB AE K\rFORWARD\tF AO R W ER D\rFULL\tF UH L\rThese files can be generated here .\nThe .kwslist file has each keyword and a certain threshold, more or less corresponding to the length of the word or phrase, as follows:\nBACK /1e-9/\rFORWARD /1e-25/\rFULL SPEED /1e-20/\rLM Language model mode additionally imposes a grammer. To utilize this mode, .dic, .lm and .gram files are needed.\nThe dictionary file is the same as in KWS mode.\nThe .lm file can be generated, along with the .dic file, from a corpus of text, using this tool\nThe generate_corpus.py script in SpeechRecognition/asr/resources sifts through the resource files from robocup\u0026rsquo;s GPSRCmdGenerator and creates a corpus. The .dic and .lm files are generated from it by using the above tool.\nFinally, the .gram file specifies the grammer to be imposed. For instance, if the commands we are expecting are always an action followed by an object or person and then a location, it might look like:\npublic \u0026lt;rule\u0026gt; = \u0026lt;actions\u0026gt; [\u0026lt;objects\u0026gt;] [\u0026lt;names\u0026gt;] [\u0026lt;locations\u0026gt;];\r\u0026lt;actions\u0026gt; = MOVE | STOP | GET | GIVE\r\u0026lt;objects\u0026gt; = BOWL | GLASS\r\u0026lt;names\u0026gt; = JOE | JOEBOB\r\u0026lt;locations\u0026gt; = KITCHEN | BEDROOM\rHandle Segmentation You can find this repository here\nTo be able to accurately pick up a bag, the robot must be able to detect where its handle is, as well as some information on how wide it is. To accomplish this, I trained a UNet model to segment images of handles.\nUNet models are models that take as input an image and output a mask defining a region of interest. Producing data for these models requires labelling regions of interest on a variety of images. For that purpose I used two tools - LableMe or in MakeSense.ai.\nTraining History for the Handle Segmentation Model\rAfter training, model inference on the test set was promising.\nModel Inference on Test Set: input image on the left, model prediction in the center and ground truth on the right\rAdditionally, some processing was done on the mask to obtain candidates for the apex of the handle, and its width. This allowed the model to output where the arm should grasp, like the sequence below. Additional work will be done to integrate the RGBD depth layer to obtain a depth location of the handle.\n2020 RoboCup@Home Education Online Challenge We (the software subteam) participated in the 2020 Online Challenge since it is the team\u0026rsquo;s goal to develop our own hardware platform for 2021. Meanwhile, we put our software progress to the test on the Turtlebot2 platform.\nOut of 8 finalists, we ended up in second place in the open category (meaning open hardware category), and first place in people\u0026rsquo;s choice.\n","date":1591395502,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395502,"objectID":"ff62214a94179c9a208c44ab9465861e","permalink":"https://Fquico1999.github.io/project/openrobotics/","publishdate":"2020-06-05T15:18:22-07:00","relpermalink":"/project/openrobotics/","section":"project","summary":"Developing Software to compete in the RoboCup@Home competition.","tags":["Deep Learning","Computer Vision"],"title":"UBC Open Robotics","type":"project"},{"authors":[],"categories":[],"content":"Overview ENPH 353 is a project course designed to teach machine learning techinques with an end-of-term competition. The premise of the competition is to develop algorithms that allow a simulated robot to traverse a parking lot and correctly identify locations and number plates of parked cars while avoiding pedestrians and a moving vehicle. The simulation took place in Gazebo in ROS\nThe Competition The image above shows the parking lot for the competition. The robot is the white, square car. It\u0026rsquo;s task is to drive on the roads while collecting the license plates on the blue rectangular cars. Additionally, it must avoid pedestrians and the truck driving around the inside track.\nExample license plate\rThe license plates hold two pieces of information, the position of the car marked with the larger P1 above, and a BC auto-generated license plate with two letters and two numbers.\nThe inputs to the system were the images captured by a camera mounted on the robot\u0026rsquo;s frame and as outputs the robot would publish velociy commands to guide the robot as well as positions and license plate data to a server for scoring.\nThe scores are determined by the following:\nRules Points Correctly record license plate and position for a car on outside track +6 Correctly record license plate and position for a car on inside track +8 More than two wheels outside of the track -2 Collide with white pick-up truck -5 Hit pedestrian -10 Drive one full loop around the outer track +5 The Strategy YOLO I decided to use the YOLO framework to allow the robot to understand it\u0026rsquo;s environment. Yolo stands for \u0026ldquo;You Only Look Once\u0026rdquo;, and is a state of the art object detection system. I used YOLOv3 to obtain labeled bounding boxes around classes of interest, namely the blue parked cars, pedestrians, the truck, and license plates.\nYOLO works by taking an image and dividing into smaller subsections, and predicting locations and accuracies for bounding boxes of a certain class. The advantage of using YOLO is that it is incredibly fast compared to other classifier models, allowing us to obtain near real-time predictions.\nTraining the model required around 200 labeled images taken from simulation video, trained for about 25000 iterations. In ROS, a node subscribes to the camera feed and passes the images through yolo. A YoloElement message was made to store each bounding box for each class, and publish it to a yolo node. This node informs pedestrian logic and gives bounding boxes for the license plate detection as well.\nYOLO Output - The robot is waiting at the cross section. It detects the pedestrian as well as the car and license plate ahead.\rNavigation The main components of the robot\u0026rsquo;s navigation are the driver and controller.\nDriver The essential method for Karen‚Äôs driving was get_twist(). This method used computer vision techniques to return a Twist command (Twist is a message that contains all the velocities of the robot) which would be called by the controller to drive the robot. The driver has three main approaches to following the road.\nThe first two approaches are very similar. The robot can follow the road by either looking at the right edge or the left edge of the road and following it. These approaches are mirror, so the following is a list of steps taken to perform right edge following:\nScale input image to a smaller size and apply HSV mask to filter out the road. Find the first pixel of a road from the right-hand side at an upper and lower location. Compare these pixel locations to the expected locations to determine the current error. If the error magnitude exceeds a threshold, turn left if the error is negative, or right if the error is positive, otherwise, drive straight. Driving Straight - the relative difference in white lines is within the threshold.\rLeft Turn - the relative difference causes a negative error, robot will turn left.\rThis method was found to be robust. Even when starting off the road, the robot will turn and navigate towards the road, and begin following the edge. However, general navigation and knowing which way to drive is not solved by this approach. The controller must solve these challenges. Note, to follow the left edge, the white lines are flipped about the y-axis in the above figures.\nThe third approach of road following is to use the ‚Äúcenter of mass‚Äù of the road. This method is not as robust as the above edge following. However, this approach is necessary when the edges of the road are misleading. This approach follows a similar idea as edge following, except it differs in steps 2 and 3:\nThreshold the image so that the road is a binary mask. Use OpenCV to compute the moments of the image, then compare the x coordinate of the center of mass of the road with the center of the image to get the error. In general, each of these approaches could follow the road successfully. It is up to the controller to decide when to use each approach.\nController The robot\u0026rsquo;s controller makes decisions about when and where to turn, when to stop for pedestrians, and when to stop for the pick-up truck. The following is a flow chart illustrating the state diagram of the controller:\nExit T-IntersectionExit T-IntersectionRight Edge Follow PerimeterRight Edge Follow\u0026hellip;NoNoSee Pedestrian?See Pedestrian?Wait Untill CrossedWait Untill CrossedYesYesCollected¬†Last Perimiter License Plate?Collected¬†Last\u0026hellip;NoNoYesYesLeft Edge Follow\n(Enter Inner Track)Left Edge Follow\u0026hellip;In Inner\nRing?In Inner\u0026hellip;NoNoFollow Road CMFollow Road CMYesYesCollected¬†Last Perimiter License Plate?Collected¬†Last\u0026hellip;FinishedFinishedNoNoYesYesTruck Close?Truck Close?Wait Untill GoneWait Untill GoneYesYesInitialize YOLO\nand License Plate ReaderInitialize YOLO\u0026hellip;Viewer does not support full SVG 1.1\nPosition and License Plate Recognition License Plates The algorithm takes cropped license plate images based on bounding box predictions from YOLO and does some preprocessing before passing them into a CNN for character recognition.\nThe preprocessing algorithm takes bounding boxes from /yolo with the license plate class and crops the raw camera image to size. We obtain potential characters using and adaptive threshold followed by cv2\u0026rsquo;s findContours() function. After some filtering based on size and aspect ratio, we end up with four predictions. The ordering of characters is determined based on the x position of the bounding box prediction.\nLicense Plate Recognition - After adaptive thresholding, findContours yields potential character candidates that are filtered producing the final 4 characters seen.\rPosition To read the positions of each license plate, a region of interest is defined based on the bounding box around the license plate from YOLO. To perform character recognition, the CNN is used again, trained on data collected from allowing the robot to do several laps around the perimeter.\nExamples of positions after cropping to ROI\rResults A total of 20 teams competed in this competition. This model was one of four to receive a perfect score of 57 points.\nThe video above shows the robot completing the outer ring. The Gazebo simulation is shown on the right, the scoring server is on the bottom left, and the terminal displaying information about the robot is on the upper left.\n","date":1591395484,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395484,"objectID":"c5502dda16902d094db7bd7a7a1a664d","permalink":"https://Fquico1999.github.io/project/enph353/","publishdate":"2020-06-05T15:18:04-07:00","relpermalink":"/project/enph353/","section":"project","summary":"Implemeted YOLO to navigate a simulated course for ENPH 353.","tags":["Deep Learning","Computer Vision"],"title":"Machine Learning Competition","type":"project"},{"authors":[],"categories":[],"content":"Overview EECE571T - Advanced Machine Learning Tools, is a graduate level machine learning course I took at UBC. A large part of this course was the final project for which I choose to do artifact removal and biomarker segmentation of FOXP3+ biomarkers for follicular lymphoma TMA cores in conjunction with the British Columbia Cancer Agency.\nThe purpose of the project was to introduce a quantitative method of evaluating FOXP3+ biomarker counts in TMA cores, and improve upon industry standard - usually estimated by eye by a Pathologist or by the software Aperio.\nOne major obstacle was the frequent presence of artifacts in the cores which would completely overpower the actual positive biomarkers themselves. These had to be ignored by Pathologists, and removed by hand in Aperio.\nAs such, the proposed framework is broken into artifact segmentation, to segment and remove artifacts, and marker segmentation to identify the biomarkers. In both cases, the input images were very large ($2886\\times 2886$), so to preserve global and local structure, patches were made and fed into UNets to produce binary masks for both artifacts and markers. These methods and results are discussed in detail in the final report paper.\nSee the paper here\n","date":1591395470,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395470,"objectID":"76a23cb2e44a62212c48b3b816da0d43","permalink":"https://Fquico1999.github.io/project/eece571t/","publishdate":"2020-06-05T15:17:50-07:00","relpermalink":"/project/eece571t/","section":"project","summary":"A Project for EECE 571T - Advanced Machine Learning Tools - Where I created a pipeline to detect FOXP3+ biomarkers in follicular lymphoma TMA cores.","tags":["Deep Learning"],"title":"Artifact Removal \u0026 Biomarker Segmentation","type":"project"}]