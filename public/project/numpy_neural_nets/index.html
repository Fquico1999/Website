<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Francisco Farinha">

  
  
  
    
  
  <meta name="description" content="Implementation of Neural Networks using only NumPy">

  
  <link rel="alternate" hreflang="en-us" href="https://Fquico1999.github.io/project/numpy_neural_nets/">

  


  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://Fquico1999.github.io/project/numpy_neural_nets/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Francisco Farinha">
  <meta property="og:url" content="https://Fquico1999.github.io/project/numpy_neural_nets/">
  <meta property="og:title" content="NumPy Neural Networks | Francisco Farinha">
  <meta property="og:description" content="Implementation of Neural Networks using only NumPy"><meta property="og:image" content="https://Fquico1999.github.io/project/numpy_neural_nets/featured.jpg">
  <meta property="twitter:image" content="https://Fquico1999.github.io/project/numpy_neural_nets/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-06-09T22:09:34-07:00">
    
    <meta property="article:modified_time" content="2020-06-09T22:09:34-07:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Fquico1999.github.io/project/numpy_neural_nets/"
  },
  "headline": "NumPy Neural Networks",
  
  "image": [
    "https://Fquico1999.github.io/project/numpy_neural_nets/featured.jpg"
  ],
  
  "datePublished": "2020-06-09T22:09:34-07:00",
  "dateModified": "2020-06-09T22:09:34-07:00",
  
  "author": {
    "@type": "Person",
    "name": "Francisco Farinha"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Francisco Farinha",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Fquico1999.github.io/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Implementation of Neural Networks using only NumPy"
}
</script>

  

  


  


  





  <title>NumPy Neural Networks | Francisco Farinha</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Francisco Farinha</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Francisco Farinha</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-center" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article article-project">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>NumPy Neural Networks</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 9, 2020
  </span>
  

  

  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 342px;">
  <div style="position: relative">
    <img src="/project/numpy_neural_nets/featured_hu4d3bb5febb14b87d984ab69f868ff292_159059_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p><em>You can find this repository 
<a href="https://github.com/Fquico1999/numpy_neural_nets" target="_blank" rel="noopener">here</a></em></p>
<h2 id="overview">Overview</h2>
<p>In an attempt to test and further my understanding of the mathematics and logistics behind neural networks and how they operate, I decided to follow what I learned in deeplearning.ai&rsquo;s 
<a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">Neural Networks and Deep Learning</a> course and implement Neural Networks from scratch using only 
<a href="https://numpy.org/" target="_blank" rel="noopener">NumPy</a>.</p>
<h2 id="outline">Outline</h2>
<p>To build a neural net from scratch, we need to go over each block and code those individually. At the end we can combine all of these to create an $L$-layer NN.</p>
<p>So, the steps we need to take are:</p>
<ul>
    <li>Parameter Intialization: We need to initialize parameters $W$ and $b$</li>
    <li>Compute a forward propagation pass: This involves computing the linear pass - $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ - and the activation $A^{[l]}=g(Z^{[l]})$ for both Sigmoid and ReLU activations</li>
    <li>Compute the loss</li>
    <li>Implement a back propagation pass</li>
    <li>Update the parameters: Here I'll code in mini Batch Gradient Descent (Which will cover both Stochastic Gradient Descent as well as Batch Gradient Descent), Momentum, RMSProp, and the king of them all, Adam</li>
</ul>
<pre><code class="language-python">import math
import numpy as np
import matplotlib.pyplot as plt
</code></pre>
<h2 id="activation-functions">Activation Functions</h2>
<p>To add non-linearity to the model, activation functions are used. I&rsquo;ll define them now.
I&rsquo;ll be using ReLU (rectified linear unit) and sigmoid in an example, but I&rsquo;ll also define tanh and leaky ReLU.</p>
<pre><code class="language-python">def relu(Z):
    &quot;&quot;&quot;
    Arguments:
    Z -- output of linear function Z = W*A+b
    
    Returns:
    ret -- ReLU(Z)
    Z -- input for use in backprop
    
    &quot;&quot;&quot;
    return np.maximum(0,Z), Z

def sigmoid(Z):
    &quot;&quot;&quot;
    Arguments:
    Z -- output of linear function Z = W*A+b
    
    Returns:
    ret -- sigmoid(Z)
    Z -- input for use in backprop
    
    &quot;&quot;&quot;
    return 1./(1.+np.exp(-Z)), Z

def tanh(Z):
    &quot;&quot;&quot;
    Arguments:
    Z -- output of linear function Z = W*A+b
    
    Returns:
    ret -- tanh(Z)
    Z -- input for use in backprop
    
    &quot;&quot;&quot;
    return np.tanh(Z), Z

def leaky_relu(Z):
    &quot;&quot;&quot;
    Arguments:
    Z -- output of linear function Z = W*A+b
    
    Returns:
    ret -- leaky_relu(Z)
    Z -- input for use in backprop
    
    &quot;&quot;&quot;
    return np.maximum(0.01*Z, Z), Z
    
    
    
</code></pre>
<h2 id="parameter-initialization">Parameter Initialization</h2>
<p>For passing parameter information between different functions, I&rsquo;ll use a dictionary <code>parameters</code>, which will store $W$ and $b$ values for each layer $l {l:{0\le l \le L}}$</p>
<p>Additionally, I&rsquo;ll implement random, Xavier initialization, and He initialization.</p>
<ul>
    <li>Random Initialization: Samples values from a normal distribution, and multiplies by a small value to keep weights close to zero - regularization</li>
    <li>Xavier Initialization: random sampling is multiplied by constant $\sqrt{\frac{1}{\text{previous layer dimension}}}$</li>
    <li>He Initialization: random sampling is multiplied by constant $\sqrt{\frac{2}{\text{previous layer dimension}}}$</li>
</ul>
<pre><code class="language-python">def initialize_parameters(model_shape, initialization_method='he'):
    &quot;&quot;&quot;
    Initializes parameters W and b of a network of shape model_shape.
    
    Arguments:
    model_shape -- list containing the dimensions of each network layer l
    
    Returns:
    parameters --  dictionary containing weight and bias parameters
    &quot;&quot;&quot;
    #define dictionary
    params = {}
    
    #Obtain L
    L = len(model_shape)
    
    #Check initialization_method
    if initialization_method == 'random':
        beta = 0.01
        for l in range(1,L):
            params[&quot;W&quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta
            params[&quot;b&quot;+str(l)] = np.zeros([model_shape[l], 1])
    
    elif initialization_method == 'xavier':
        L = L-1
        for l in range(1,L+1):
            beta = np.sqrt(1./model_shape[l-1])
            params[&quot;W&quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta
            params[&quot;b&quot;+str(l)] = np.zeros([model_shape[l], 1])
    
    elif initialization_method == 'he':
        L = L - 1
        for l in range(1,L+1):
            beta = np.sqrt(2./model_shape[l-1])
            params[&quot;W&quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta
            params[&quot;b&quot;+str(l)] = np.zeros([model_shape[l], 1])
    else:
        raise NameError(&quot;%s is not a valid initalization method&quot;%(initialization_method))

    return params
</code></pre>
<h2 id="forward-propagation">Forward Propagation</h2>
<p>Forward propagation refers to passing through the computation graph from left to right - forwards - and evaluating $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ for each sucessive $l$ starting with $l=1$, in which case $A^{[0]}=X$, in other words, the activation fed into the first layer is simply the inputs.</p>
<p>To accomplish this, I&rsquo;ll create two functions. The first will evaluate the linear formula $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$, whereas the second will evaluate $A^{[l]} = g(Z^{[l]})$, which corresponds to evaluating the activation function.</p>
<p>Then <code>forward_prop</code> implements both to complete a forward propagation pass.</p>
<p>In order to compute the backprop later onwards, I&rsquo;ll need to store $A^{[l]}$,$W^{[l]}$, $b^{[l]}$ as well as $Z^{[l]}$ which I&rsquo;ll do in <code>linear cache</code> and <code>activation cache</code></p>
<p>One of the arguments of <code>forward_prop</code> is <code>layer_activations</code>, which is a list of the activations for each layer of the neural network.</p>
<pre><code class="language-python">def forward_linear(W,A,b):
    &quot;&quot;&quot;
    Linear part of forward propagation

    Arguments:
    W -- weight matrix
    A -- activations
    b -- bias matrix

    Returns:
    Z -- input to the layer's activation function
    linear_cache -- tuple with A, W, b for efficient backprop
    &quot;&quot;&quot;
    Z = np.dot(W,A)+b
    
    linear_cache = (A,W,b)
    
    assert(Z.shape == (W.shape[0], A.shape[1]))
    
    return Z, linear_cache
</code></pre>
<pre><code class="language-python">def forward_activation(Z, activation):
    &quot;&quot;&quot;
    Arguments:
    Z -- Output of linear function Z = WA_prev+b
    activation -- String denoting activation function to use. One of [linear, sigmoid, relu, leaky_relu, tanh, softmax]
    
    Returns:
    A -- g(Z), where g() is the corresponding activation
    activation_cache -- the input Z, which will be fed into backprop
    &quot;&quot;&quot;
    
    if activation == 'linear':
        A, activation_cache = Z, Z
    elif activation == 'sigmoid':
        A, activation_cache = sigmoid(Z)
    elif activation == 'relu':
        A, activation_cache = relu(Z)
    elif activation == 'leaky_relu':
        A, activation_cache = leaky_relu(Z)
    elif activation == 'tanh':
        A, activation_cache = tanh(Z)
    else:
        raise NameError('%s is not a valid activation function' %(activation))
    
    return A, activation_cache
</code></pre>
<pre><code class="language-python">def forward_prop(X, layer_activations, parameters):
    &quot;&quot;&quot;
    Implements one pass of forward propagation
    
    Arguments:
    X -- input data
    layer_activations -- list of strings corresponding to the activations of each layer
    parameters -- output of initialize_parameters
    
    Returns:
    A - Output of activation function of the last layer
    caches - list of caches containing both linear and activation caches
    &quot;&quot;&quot;
    #Define caches
    caches = []
    #A[0] is the input
    A = X
    L = len(parameters)//2 
    
    for l in range(1, L+1):
        A_prev = A
        W = parameters[&quot;W&quot;+str(l)]
        b = parameters[&quot;b&quot;+str(l)]
        Z, linear_cache = forward_linear(W, A_prev, b)
        A, activation_cache = forward_activation(Z, layer_activations[l])
        
        assert (A.shape == (W.shape[0], A_prev.shape[1]))
        
        #Add both linear and activation cache to caches
        caches.append((linear_cache, activation_cache))

    return A, caches
</code></pre>
<h2 id="cost-function">Cost Function</h2>
<p>The cost function is the metric that a neural net aims to minimize. I&rsquo;ll implement cross-entropy cost, given by:</p>
<p>$$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))$$</p>
<p>Thus, we require a method of computing cost after one pass of forward propagation.</p>
<pre><code class="language-python">def cost(A_last, Y):
    &quot;&quot;&quot;
    Arguments:
    A_last -- Post-activation value of the last layer of the network
    Y -- Groud truth vectors
    
    Returns:
    cost -- cross-entropy cost
    &quot;&quot;&quot;
    #Get number of samples, m
    m = Y.shape[1]
    #Compute cross entropy cost
    cost = -(1.0/m)*np.sum(Y*np.log(A_last)+(1.-Y)*np.log(1.-A_last))
        
    #Ensure appropriate dimensions
    cost = np.squeeze(cost)
    
    return cost
    
</code></pre>
<h2 id="back-propagation">Back Propagation</h2>
<p>To update our parameters, we need to calculate the gradient of the loss with respect to $W$ and $b$</p>
<p>Just like with forward prop, I will implement two functions. One deals with the back pass for the linear part of the units and the other deals with the derivatives of the activation functions.</p>
<p>For the linear part, we take the derivatives of the parameters, obtaining:</p>
<p>$$ dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} $$
$$ db^{[l]} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}$$
$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} $$</p>
<p>For the activation part, the backprop requires the gradient of the activation function. As such it depends on the activation used, and I&rsquo;ll define them for each one.</p>
<p>For sigmoid:</p>
<p>$$ \sigma{(z)} = \frac{1}{1+e^{-x}}$$
$$\frac{d\sigma{(z)}}{dz} = \sigma{(z)}(1-\sigma{(z)})$$</p>
<p>For ReLU:</p>
<p>$$\text{ReLU}(z) = \max{(0,z)}$$
$$\frac{d\text{ReLU}}{dz} = \left\{\begin{array}{ll}1 , z &gt; 0\\0, z \le 0\end{array}\right.$$</p>
<p>Note that for ReLU, strictly speaking, there is a discontinuity at $z=0$, however since it is incredibly unlikely that the input to the function will every be exactly zero, it&rsquo;s fine to include it in  $z\le0$</p>
<p>For tanh:
$$\tanh{(z)} = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$
$$\frac{d\tanh(z)}{dz} = 1-\tanh^2(z)$$</p>
<p>For leaky ReLU:
$$\text{leaky ReLU}(z) = \max(0.01z, z)$$
$$\frac{d(\text{leaky Relu}(z))}{dz} = \left\{\begin{array}{ll}1 , z &gt; 0\\0.01, z \le0\end{array}\right.$$</p>
<p>So, I&rsquo;ll implement functions for each of these units to compute:
$$dZ^{[l]} = dA^{[l]} * g&rsquo;(Z^{[l]})$$</p>
<p>Additionally, to initialize backpropagation, we need $\frac{d\mathcal{L}}{dA^{[L]}}$, the gradient of the cost function with respect to the last activation output. For cross-entropy this is:
$$-\sum\limits_{i=1}^{m}\frac{y^{i}}{a^{[L](i)}} - \frac{1-y^{i}}{1-a^{[L](i)}}$$</p>
<pre><code class="language-python">def backward_linear(dZ, cache):
    &quot;&quot;&quot;
    Arguments:
    dZ -- Gradient of cost w.r.t linear portion
    cache -- tuple coming from cached forward prop of layer l
    
    Returns:
    dA_prev -- gradient with respect to activation of previous layer
    dW -- gradient with respect to weights of current layer
    db -- gradient with respect to biases of current layer
    &quot;&quot;&quot;
    
    #unpack cache
    A_prev, W, b = cache
    #Get number of samples
    m = A_prev.shape[1]
    
    dW = 1./m*np.dot(dZ, A_prev.T)
    db = 1./m*np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)
    
    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)
    
    return dA_prev, dW, db
</code></pre>
<pre><code class="language-python">def backward_activation(dA, Z, activation):
    &quot;&quot;&quot;
    Arguments:
    dA -- post-activation gradient for current layer l 
    Z -- cached matrix from forward prop
    activation -- the activation to be used in the layer
    
    Returns:
    dZ -- gradient of cost function with respect to Z[l]
    &quot;&quot;&quot;
    
    if activation == 'linear':
        dZ = dA
    
    elif activation == &quot;relu&quot;:
        dZ = np.array(dA, copy=True)
        dZ[Z &lt;= 0] = 0
        
    elif activation == &quot;sigmoid&quot;:
        s = 1./(1+np.exp(-Z))
        dZ = dA * s * (1-s)

    elif activation == &quot;leaky_relu&quot;:
        dZ = np.array(dA, copy=True)
        dZ[Z &lt;= 0] = 0.01

    elif activation == &quot;tanh&quot;:
        dZ = dA*(1 - tanh(Z)**2)
    
    else:
        raise NameError(&quot;%s is not a valid activation function&quot; % (activation))
    assert(dZ.shape == Z.shape)
    return dZ
    
</code></pre>
<pre><code class="language-python">def backward_prop(AL, Y, caches, layer_activations):
    &quot;&quot;&quot;
    Implement a backward propagation pass
    
    Arguments:
    AL -- output of the forward propagation
    Y -- ground truth
    caches -- list of caches containing linear_cache and activation_cache
    
    Returns:
    grads -- A dictionary with the gradients dA[l], dW[l], db[l]
    &quot;&quot;&quot;
    
    #Define dict to store gradients for parameter update
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    #Ensure Y is the same as AL (which is essentially y_hat)
    Y = Y.reshape(AL.shape)
    
    #Initialize backprop, a.k.a derivative of cost with respect to AL
    dAL =  -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    grads[&quot;dA&quot;+str(L)] = dAL

    for l in reversed(range(L)):
        current_cache = caches[l]
        linear_cache, activation_cache = current_cache
        dZ = backward_activation(grads[&quot;dA&quot;+str(l+1)],activation_cache, layer_activations[l])
        dA_prev, dW, db = backward_linear(dZ, linear_cache)

        grads[&quot;dA&quot; + str(l)] = dA_prev
        grads[&quot;dW&quot; + str(l + 1)] = dW
        grads[&quot;db&quot; + str(l + 1)] = db
    
    return grads
</code></pre>
<h2 id="update-parameters">Update Parameters</h2>
<p>The final step is to take the gradients computed in back propagation and use them to update the parameters $W$ and $b$.</p>
<p>The method of updating these parameters is important and there are several optimizers that do this in different ways.</p>
<ul>
<li>Mini-Batch Gradient Descent:
$$ W:=W-\alpha dW $$
$$ b:=b-\alpha db $$</li>
</ul>
<p>For the other optimization algorithms, the concept of exponentially weighted averages becomes an important one. An exponentially weighted average can be calculated with the following formula:
$$v_{\theta, i} := \beta v_{\theta, i} + (1-\beta)\theta_{i}$$</p>
<p>Where $\theta_{i}$ are the samples in the dataset to average over. The parameter $\beta$ roughly controls how many samples to average over given by approximately $\frac{1}{1-\beta}$. Most commonly in momentum, $\beta=0.9$, which works out to averaging over the last 10 samples.</p>
<ul>
<li>Momentum:
$$ \begin{cases}
v_{dW^{[l]}} := \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\
W^{[l]} := W^{[l]} - \alpha v_{dW^{[l]}}
\end{cases}$$</li>
</ul>
<p>$$\begin{cases}
v_{db^{[l]}} := \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\
b^{[l]} := b^{[l]} - \alpha v_{db^{[l]}}
\end{cases}$$</p>
<ul>
<li>RMSProp:
$$ \begin{cases}
s_{dW^{[l]}} := \beta s_{dW^{[l]}} + (1 - \beta) (dW^{[l]})^{2} \\
W^{[l]} := W^{[l]} - \alpha \frac{dW^{[l]}}{\sqrt{s_{dW^{[l]}}}+\epsilon}
\end{cases}$$</li>
</ul>
<p>$$\begin{cases}
s_{db^{[l]}} := \beta s_{db^{[l]}} + (1 - \beta) (db^{[l]})^{2} \\
b^{[l]} := b^{[l]} - \alpha \frac{db^{[l]}}{\sqrt{s_{db^{[l]}}}+\epsilon}
\end{cases}$$</p>
<p>Note the addition of $\epsilon$ in the denominator in both RMSProp and Adam. That is to prevent NaNs or divisions by zero, it increases numerical stability. The king of the optimizers, Adam, works by combining both momentum and RMSProp. Additionally, it also adds bias correction to the exponentially weighted averages $v$ and $s$. The need for bias correction comes from the fact that as the number of samples that we average over increases, the beginning of the averaging causes the output to be very small since at the start we only have one sample and the others are initialized to zero. As such, the start of our averaging results in a much lower start than the original distribution.</p>
<ul>
<li>Adam:
$$\begin{cases}
v_{dW^{[l]}} := \beta_1 v_{dW^{[l]}} + (1 - \beta_1) dW^{[l]} \\
v^{corrected}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \\
s_{dW^{[l]}} := \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (dW^{[l]})^2 \\
s^{corrected}<em>{dW^{[l]}} = \frac{s</em>{dW^{[l]}}}{1 - (\beta_2)^t} \\
W^{[l]} := W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}
\end{cases}$$</li>
</ul>
<p>$$\begin{cases}
v_{db^{[l]}} := \beta_1 v_{db^{[l]}} + (1 - \beta_1) db^{[l]} \\
v^{corrected}<em>{db^{[l]}} = \frac{v</em>{db^{[l]}}}{1 - (\beta_1)^t} \\
s_{db^{[l]}} := \beta_2 s_{db^{[l]}} + (1 - \beta_2) (db^{[l]})^2 \\
s^{corrected}<em>{db^{[l]}} = \frac{s</em>{db^{[l]}}}{1 - (\beta_2)^t} \\
b^{[l]} := b^{[l]} - \alpha \frac{v^{corrected}<em>{db^{[l]}}}{\sqrt{s^{corrected}</em>{db^{[l]}}} + \varepsilon}
\end{cases}$$</p>
<p>The $t$ parameter in Adam included in the bias correction formula is the number of steps taken.</p>
<p>Besides functions to update these parameters, we also need functions to initialize them (except for gradient descent)</p>
<pre><code class="language-python">## Gradient Descent

def update_parameters_gd(parameters, grads, learning_rate=0.01):
    &quot;&quot;&quot;
    Arguments:
    parameters -- parameters W and b
    grads -- gradients from backprop - dW and db
    
    Returns:
    parameters -- parameters W and b updated using gradient descent update rules
    &quot;&quot;&quot;
    L = len(parameters) // 2 # number of layers

    for l in range(L):
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)] - learning_rate*grads[&quot;dW&quot;+str(l+1)]
        parameters[&quot;b&quot; + str(l+1)] = parameters[&quot;b&quot; + str(l+1)]- learning_rate*grads[&quot;db&quot;+str(l+1)]
    
    return parameters

## Momentum

def initialize_parameters_momentum(parameters):
    &quot;&quot;&quot;
    Arguments:
    parameters -- dictionary containing parameters W,b
    
    Returns:
    velocities -- initialized velocities for momentum updates
    &quot;&quot;&quot;
    
    L = len(parameters) // 2
    velocities = {}
    
    # Initialize velocities
    for l in range(L):
        velocities[&quot;dW&quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)
        velocities[&quot;db&quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)
        
    return velocities

def update_parameters_momentum(parameters, grads, velocities, learning_rate=0.01, beta=0.9):
    &quot;&quot;&quot;
    Arguments:
    parameters -- parameters W and b
    grads -- gradients from backprop - dW and db
    velocities -- current velocities for momentum
    
    Returns:
    parameters -- parameters W and b updated using momentum update rules
    velocities -- updated velocities
    &quot;&quot;&quot;
    L = len(parameters) // 2
    
    for l in range(L):
        # compute velocities using exponential weighted average
        velocities[&quot;dW&quot; + str(l+1)] = beta*velocities[&quot;dW&quot;+str(l+1)]+(1-beta)*grads[&quot;dW&quot;+str(l+1)]
        velocities[&quot;db&quot; + str(l+1)] = beta*velocities[&quot;db&quot;+str(l+1)]+(1-beta)*grads[&quot;db&quot;+str(l+1)]

        #parameter update
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)] - learning_rate*velocities[&quot;dW&quot; + str(l+1)]
        parameters[&quot;b&quot; + str(l+1)] = parameters[&quot;b&quot; + str(l+1)] - learning_rate*velocities[&quot;db&quot; + str(l+1)]
        
    return parameters, velocities

## RMSProp
def initialize_parameters_rmsprop(parameters):
    &quot;&quot;&quot;
    Arguments:
    parameters -- dictionary containing parameters W,b
    
    Returns: 
    squares -- initialized moving average of the squared gradient for rmsprop updates
    &quot;&quot;&quot;
    
    L = len(parameters) // 2 
    squares = {}

    # Initialize squares
    for l in range(L):
        squares[&quot;dW&quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)
        squares[&quot;db&quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)
    
    return squares
    

def update_parameters_rmsprop(parameters, grads, squares, learning_rate=0.01,
                              beta=0.9, epsilon=1e-8):
    &quot;&quot;&quot;
    Arguments:
    parameters -- parameters W and b
    grads -- gradients from backprop - dW and db
    squares -- current squres of past gradients for rmsprop
    
    Returns:
    parameters -- parameters W and b updated using rmsprop update rules
    squares -- updated squares
    &quot;&quot;&quot;
    L = len(parameters) // 2
    
    for l in range(L):
        # compute velocities using exponential weighted average
        squares[&quot;dW&quot; + str(l+1)] = beta*squares[&quot;dW&quot;+str(l+1)]+(1-beta)*(grads[&quot;dW&quot;+str(l+1)]**2)
        squares[&quot;db&quot; + str(l+1)] = beta*squares[&quot;db&quot;+str(l+1)]+(1-beta)*(grads[&quot;db&quot;+str(l+1)]**2)

        #parameter update
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)] - learning_rate*(grads[&quot;dW&quot;+str(l+1)]/(np.sqrt(squares[&quot;dW&quot; + str(l+1)])+epsilon))
        parameters[&quot;b&quot; + str(l+1)] = parameters[&quot;b&quot; + str(l+1)] - learning_rate*(grads[&quot;db&quot;+str(l+1)]/(np.sqrt(squares[&quot;db&quot; + str(l+1)])+epsilon))
        
    return parameters, squares

## Adam
def initialize_parameters_adam(parameters):
    &quot;&quot;&quot;
    Arguments:
    parameters -- dictionary containing parameters W,b
    
    Returns: 
    velocities -- initialized first gradient weighted averages for adam updates
    squares -- initialized moving average of the squared gradient for adam updates
    &quot;&quot;&quot;
    
    L = len(parameters) // 2 
    velocities = {}
    squares = {}

    # Initialize velocities and squares
    for l in range(L):
        velocities[&quot;dW&quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)
        velocities[&quot;db&quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)
        squares[&quot;dW&quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)
        squares[&quot;db&quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)
    
    return velocities, squares

def update_parameters_adam(parameters, grads, velocities, squares, t, learning_rate=0.01,
                           beta1=0.9, beta2=0.999, epsilon=1e-8):
    &quot;&quot;&quot;
    Arguments:
    parameters -- dictionary with parameters W, b
    grads -- dictionary with gradients dW, db
    velocities -- moving average of the first gradient
    squares -- moving average of the squared gradient
    t -- counter for bias correction
    
    Returns:
    parameters -- updated parameters according to adam
    velocities -- updated moving average of the first gradient
    squares -- updated moving average of the squared gradient
    &quot;&quot;&quot;
    L = len(parameters) // 2                 
    v_corrected = {}                        
    s_corrected = {}                         

    for l in range(L):
        #Calculate exponentially weighted velocities
        velocities[&quot;dW&quot; + str(l+1)] = beta1*velocities[&quot;dW&quot; + str(l+1)]+(1-beta1)*grads[&quot;dW&quot; + str(l+1)]
        velocities[&quot;db&quot; + str(l+1)] = beta1*velocities[&quot;db&quot; + str(l+1)]+(1-beta1)*grads[&quot;db&quot; + str(l+1)]

        #Bias correction for velocities
        v_corrected[&quot;dW&quot; + str(l+1)] = velocities[&quot;dW&quot; + str(l+1)]/(1-beta1**t)
        v_corrected[&quot;db&quot; + str(l+1)] = velocities[&quot;db&quot; + str(l+1)]/(1-beta1**t)
    
        #Calculate exponentially weighted squares
        squares[&quot;dW&quot; + str(l+1)] = beta2*squares[&quot;dW&quot; + str(l+1)]+(1-beta2)*grads[&quot;dW&quot; + str(l+1)]**2
        squares[&quot;db&quot; + str(l+1)] = beta2*squares[&quot;db&quot; + str(l+1)]+(1-beta2)*grads[&quot;db&quot; + str(l+1)]**2
        
        #Bias correction for squares
        s_corrected[&quot;dW&quot; + str(l+1)] = squares[&quot;dW&quot; + str(l+1)]/(1-beta2**t)
        s_corrected[&quot;db&quot; + str(l+1)] = squares[&quot;db&quot; + str(l+1)]/(1-beta2**t)
    
        #Adam parameter updates
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)] - learning_rate*(v_corrected[&quot;dW&quot; + str(l+1)]/(np.sqrt(s_corrected[&quot;dW&quot; + str(l+1)])+epsilon))
        parameters[&quot;b&quot; + str(l+1)] = parameters[&quot;b&quot; + str(l+1)] - learning_rate*(v_corrected[&quot;db&quot; + str(l+1)]/(np.sqrt(s_corrected[&quot;db&quot; + str(l+1)])+epsilon))

    return parameters, velocities, squares
</code></pre>
<h2 id="combining-everything-and-mini-batch-gd">Combining Everything and Mini-Batch GD</h2>
<p>After going through each piece, we now need to combine all these functions to train a model.
To do this, we have some input data $X$ with respective labels $Y$. Now, to implement mini-bach gradient descent, we need to split $X$ and $Y$ into $m$ mini-batches to run our algorithms on.</p>
<pre><code class="language-python">def mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    &quot;&quot;&quot;
    Arguments:
    X -- input data
    Y -- corresponding labels
    mini_batch_size -- size of the mini-batches
    seed -- used to set np.random.seed differently to get different shuffles
    
    Returns:
    mini_batches -- list of (mini_batch_X, mini_batch_Y)
    &quot;&quot;&quot;
    
    #Set seed
    np.random.seed(seed)
    
    mini_batches = []
    #Get number of examples
    m = X.shape[1] 

    idx = list(np.random.permutation(m))
    shuffled_X = X[:, idx]
    shuffled_Y = Y[idx, :]
    shuffled_Y = np.reshape(shuffled_Y,(1,m))
    
    assert shuffled_Y.shape == (1,m)
    
    #Need to account for when minibatch size is divisible by m 
    num_full_minibatch = int(math.floor(m/mini_batch_size))
    for i in range(0, num_full_minibatch):

        mini_batch_X = shuffled_X[:,mini_batch_size*i: mini_batch_size*(i+1)]
        mini_batch_Y = shuffled_Y[:,mini_batch_size*i: mini_batch_size*(i+1)]
        mini_batches.append((mini_batch_X, mini_batch_Y))
    
    # Now need to take care of extra examples of len &lt; m
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[:,-(mini_batch_size-m):]
        mini_batch_Y = shuffled_Y[:,-(mini_batch_size-m):]
        mini_batches.append((mini_batch_X, mini_batch_Y))
    
    return mini_batches
</code></pre>
<pre><code class="language-python">def train(X, Y, model_shape, layer_activations, optimizer, initialization_method='he', learning_rate = 0.001, mini_batch_size = 64, beta = 0.9,
          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):
    &quot;&quot;&quot;
    Implementation of a Neural Network model.
    
    Arguments:
    X -- input data
    Y -- labels
    model_shape -- python list with the size of each layer
    layer_activations -- python list with activation of each layer
    optimizer -- string corresponding to optimizer to use. One of &quot;gd&quot;,&quot;momentum&quot;,&quot;rmsprop&quot;,&quot;adam&quot;
    learning_rate -- the learning rate parameter
    mini_batch_size -- the size of each mini batch
    beta -- Momentum/RMSProp hyperparameter
    beta1 -- decay of past gradients parameter for adam
    beta2 -- decay of past squared gradients for adam
    epsilon -- hyperparameter preventing division by zero in Adam and RMSProp updates
    num_epochs -- number of epochs
    print_cost -- True to print the cost every 5 epochs

    Returns:
    parameters -- trained parameters
    &quot;&quot;&quot;
    
    #Track costs
    costs = []  
    
    #Adam bias correction parameter
    t = 0                           
    
    #define seed for np.random.seed in mini_batch call
    seed = np.random.randint(1000)
    
    #Number of layers and number of training examples
    L = len(model_shape)
    m = X.shape[1]
    
    # Initialize parameters
    parameters = initialize_parameters(model_shape, initialization_method=initialization_method)

    # Initialize parameters for optimizer
    if optimizer == &quot;gd&quot;:
        pass
    elif optimizer == &quot;momentum&quot;:
        velocities = initialize_parameters_momentum(parameters)
    elif optimizer == 'rmsprop':
        squares = initialize_parameters_rmsprop(parameters)
    elif optimizer == &quot;adam&quot;:
        velocities, squares = initialize_parameters_adam(parameters)
    else:
        raise NameError(&quot;%s is not a valid optimizer&quot; % (optimizer))
    
    #Loop
    for i in range(num_epochs):
        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch
        seed = seed + 1
        minibatches = mini_batches(X, Y, mini_batch_size, seed)

        #Get cost over all batchs
        total_cost = 0

        for minibatch in minibatches:

            # Unpack
            (minibatch_X, minibatch_Y) = minibatch

            # Forward propagation pass
            AL, caches = forward_prop(minibatch_X, layer_activations, parameters)

            #Get minibatch cost
            cost_batch = cost(AL, minibatch_Y)

            #Add to total cost
            total_cost+=cost_batch

            # Backward propagation pass
            grads = backward_prop(AL, minibatch_Y, caches, layer_activations)
            
            # Update parameters
            if optimizer == &quot;gd&quot;:
                parameters = update_parameters_gd(parameters, grads, learning_rate=learning_rate)
            elif optimizer == &quot;momentum&quot;:
                parameters, velocities = update_parameters_momentum(parameters,grads,
                                                                    velocities,learning_rate=learning_rate,
                                                                    beta=beta)
            elif optimizer == &quot;rmsprop&quot;:
                parameters, squares = update_parameters_rmsprop(parameters, grads, squares,
                                                                learning_rate=learning_rate, beta=beta,
                                                                epsilon=epsilon)
            elif optimizer == &quot;adam&quot;:
                #Increment bias correction parameter
                t = t + 1
                parameters, velocities, squares = update_parameters_adam(parameters, grads,
                                                                         velocities, squares,
                                                               t, learning_rate=learning_rate,
                                                                         beta1=beta1, beta2=beta2,
                                                                         epsilon=epsilon)
        mean_cost = total_cost / float(mini_batch_size)
        
        # Print the cost every 5 epoch
        if print_cost and i % 5 == 0:
            print (&quot;Cost after epoch %i: %f&quot; %(i, mean_cost))
        if print_cost and i % 1 == 0:
            costs.append(mean_cost)
                
    # plot the cost
    fig, ax = plt.subplots()
    fig.set_facecolor('w')
    fig.set_size_inches(12,9)
    ax.plot(costs)
    ax.set_ylabel('Cost')
    ax.set_xlabel('Epoch')
    plt.title(&quot;Learning rate = %s, Optimizer = %s&quot; % (learning_rate, optimizer))
    plt.show()

    return parameters
</code></pre>
<h2 id="testing-the-model">Testing the Model</h2>
<p>Now that the implementation is complete, let&rsquo;s test the model by doing binary classification on two handwritten digits from the MNIST dataset.</p>
<pre><code class="language-python">from tensorflow.keras.datasets import mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

img_shape = X_train.shape[1:]

print('X_train has shape %s\nY_train has shape %s'%(X_train.shape, Y_train.shape))
</code></pre>
<pre><code>X_train has shape (60000, 28, 28)
Y_train has shape (60000,)
</code></pre>
<pre><code class="language-python">#Convert Y_train and Y_test to (m,1)
Y_train = Y_train.reshape(Y_train.shape[0],1)
Y_test = Y_test.reshape(Y_test.shape[0],1)

#Visualize one Entry
i = np.random.randint(X_train.shape[0])

fig,ax = plt.subplots()
fig.set_facecolor('w')

ax.imshow(X_train[i])
ax.set_title('Label = ' + str(Y_train[i]))
plt.show()
</code></pre>
<p><img src="./numpy_neural_nets_23_0.png" alt="png"></p>
<pre><code class="language-python">#Choose two classes for our classification model
class_a = 3 #Positive Class
class_b = 7 #Negative Class

#Filter out the dataset to include only images in those classes
idx = np.logical_or(np.squeeze(Y_train) == class_a, np.squeeze(Y_train) == class_b)
X_train, Y_train = X_train[idx], Y_train[idx]
#Assign class_a = 1 and class_b=0
Y_train[np.where(Y_train == class_a)] = 1.00
Y_train[np.where(Y_train == class_b)] = 0.00

print('X_train has shape %s\nY_train has shape %s'%(X_train.shape, Y_train.shape))

idx = np.logical_or(np.squeeze(Y_test) == class_a, np.squeeze(Y_test) == class_b)
X_test, Y_test = X_test[idx], Y_test[idx].astype(np.float64)
#Assign class_a = 1 and class_b=0
Y_test[np.where(Y_test == class_a)] = 1.00
Y_test[np.where(Y_test == class_b)] = 0.00
print('X_test has shape %s\nY_test has shape %s'%(X_test.shape, Y_test.shape))
</code></pre>
<pre><code>X_train has shape (12396, 28, 28)
Y_train has shape (12396, 1)
X_test has shape (2038, 28, 28)
Y_test has shape (2038, 1)
</code></pre>
<pre><code class="language-python">#Reshape X_train and X_test into (m, 28*28)
X_train_flat = X_train.reshape(X_train.shape[0], -1).T  
X_test_flat = X_test.reshape(X_test.shape[0], -1).T

# Standardize data to have feature values between 0 and 1.
X_train_norm = X_train_flat/255.
X_test_norm = X_test_flat/255.

print (&quot;X_train's shape: &quot; + str(X_train_norm.shape))
print (&quot;X_test's shape: &quot; + str(X_test_norm.shape))
</code></pre>
<pre><code>X_train's shape: (784, 12396)
X_test's shape: (784, 2038)
</code></pre>
<h3 id="defining-our-model">Defining our Model</h3>
<p>I&rsquo;ve chosen to create a model to classify either a $3$ or a $7$. Now, let&rsquo;s define a model.</p>
<p>The output is either $1$ or $0$, where $1$ corresponds to a $3$ and $0$ corresponds to a $7$. This means the last layer dimension needs to be $1$.
For the first dimension, that should be $28\times28\times1=784$, since we&rsquo;re taking the image and stacking each row of pixels ontop of each other (flattening).
For our hidden layer, I&rsquo;ll choose $n_h=7$
So we have a three layer model - $784\times7\times1$ with layer activations ReLU-ReLU-Sigmoid.</p>
<p>We can compare the performance of gradient descent versus adam optimization. Let&rsquo;s start with gradient descent.</p>
<pre><code class="language-python">#Model Parameters
n_x = X_train_norm.shape[0]
n_y = 1
n_h = 7
model_shape = (n_x, n_h, n_y)

layer_activations = ['relu','relu','sigmoid']
optimizer = 'gd'

learning_rate = 0.0005

parameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer,
                   learning_rate=learning_rate, mini_batch_size=128, num_epochs=17)
</code></pre>
<pre><code>Cost after epoch 0: 0.458453
Cost after epoch 5: 0.259276
Cost after epoch 10: 0.162094
Cost after epoch 15: 0.085669
</code></pre>
<p><img src="./numpy_neural_nets_27_1.png" alt="png"></p>
<h2 id="evaluating-our-model">Evaluating our Model</h2>
<p>Now that the model has trained, we need some way of assessing the performance of our model. This is done with our testing set: $(X_{\text{test}}, Y_{\text{test}})$
Essentially, we just need to feed $X_{\text{test}}$ through our model&rsquo;s forward pass, which outputs $A^{[L]}=\hat{Y}$, our predictions. Then we simply compare $\hat{Y}$ with $Y_{\text{test}}$ and evaluate the accuracy as $A=\frac{\text{# correct}}{\text{# total}}$.
Additionally, I&rsquo;ll return the indices where the model predicted correctly, and where it predicted incorrectly, to visualize the model&rsquo;s shortcomings.</p>
<pre><code class="language-python">def evaluate(X_test, Y_test, layer_activations, parameters, threshold=0.5):
    &quot;&quot;&quot;
    Evaluates performance of trained model on test set
    
    Attributes:
    X_test -- Test set inputs
    Y_test -- Test set labels
    layer_activations -- python list of strings corresponding to activation functions of layer l
    parameters -- trained parameters W, b
    
    Returns:
    correct -- list of booleans corresponding to the indices of correct predictions
    incorrect -- list of booleans correspondingin to the indices of incorrect predictions
    &quot;&quot;&quot;
    
    #Number of test samples
    m = X_test.shape[1]
    
    assert Y_test.shape == (1,m)
    
    Y_pred, _ = forward_prop(X_test, layer_activations, parameters)
    
    #Threshold
    Y_pred[Y_pred&gt;threshold]=1.
    Y_pred[Y_pred&lt;=threshold]=0
    
    num_correct = np.sum(Y_pred == Y_test)
    num_incorrect = m-num_correct
    print(&quot;Accuracy: %f&quot; % (float(num_correct)/m))
    
    correct = Y_pred == Y_test
    incorrect = Y_pred != Y_test
    
    return np.squeeze(correct), np.squeeze(incorrect)
</code></pre>
<pre><code class="language-python">#Evaluate
correct, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters)

#Get correect predictions
X_correct = X_test[correct]
Y_correct = Y_test[correct]

#Get incorrect predictions
X_incorrect = X_test[incorrect]
Y_incorrect = Y_test[incorrect]

fig,ax = plt.subplots(3,2)
fig.set_size_inches(12,18)
fig.set_facecolor('w')

i_correct = np.random.randint(len(X_correct), size=3)
i_incorrect = np.random.randint(len(X_incorrect), size=3)

for i in range(3):
    ax[i,0].imshow(X_correct[i_correct[i]])
    ax[i,0].set_title(&quot;%i: Correctly predicted Y=%i&quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b))
    
    ax[i,1].imshow(X_incorrect[i_incorrect[i]])
    ax[i,1].set_title(&quot;%i: Incorrectly predicted Y=%i&quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a))
    
    ax[i,0].xaxis.set_visible(False)
    ax[i,0].yaxis.set_visible(False)
    ax[i,1].xaxis.set_visible(False)
    ax[i,1].yaxis.set_visible(False)

plt.show()
</code></pre>
<pre><code>Accuracy: 0.964671
</code></pre>
<p><img src="./numpy_neural_nets_30_1.png" alt="png"></p>
<h3 id="adam-optimization">Adam Optimization</h3>
<p>Now that we&rsquo;ve gotten results using Gradient Descent, Let&rsquo;s compare it with adam optimization</p>
<pre><code class="language-python">#Model Parameters
n_x = X_train_norm.shape[0]
n_y = 1
n_h = 7
model_shape = (n_x, n_h, n_y)

layer_activations = ['relu','relu','sigmoid']
optimizer = 'adam'

learning_rate = 0.0005

parameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer,
                   learning_rate=learning_rate, mini_batch_size=128, num_epochs=5)
</code></pre>
<pre><code>Cost after epoch 0: 0.347253
</code></pre>
<p><img src="./numpy_neural_nets_32_1.png" alt="png"></p>
<pre><code class="language-python">#Evaluate
correct, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters)

#Get correect predictions
X_correct = X_test[correct]
Y_correct = Y_test[correct]

#Get incorrect predictions
X_incorrect = X_test[incorrect]
Y_incorrect = Y_test[incorrect]

fig,ax = plt.subplots(3,2)
fig.set_size_inches(12,18)
fig.set_facecolor('w')

i_correct = np.random.randint(len(X_correct), size=3)
i_incorrect = np.random.randint(len(X_incorrect), size=3)

for i in range(3):
    ax[i,0].imshow(X_correct[i_correct[i]])
    ax[i,0].set_title(&quot;%i: Correctly predicted Y=%i&quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b))
    
    ax[i,1].imshow(X_incorrect[i_incorrect[i]])
    ax[i,1].set_title(&quot;%i: Incorrectly predicted Y=%i&quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a))
    
    ax[i,0].xaxis.set_visible(False)
    ax[i,0].yaxis.set_visible(False)
    ax[i,1].xaxis.set_visible(False)
    ax[i,1].yaxis.set_visible(False)
plt.show()
</code></pre>
<pre><code>Accuracy: 0.969578
</code></pre>
<p><img src="./numpy_neural_nets_33_1.png" alt="png"></p>
<h3 id="comparison">Comparison</h3>
<p>As we can see, using the adam optimizer yielded better accuracy in nearly one third of the number of epochs.</p>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
</div>














  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/francisco-farinha/avatar_hu018d595a7f8c06b17e4d5128358561ce_542344_270x270_fill_q90_lanczos_center.jpg" alt="Francisco Farinha">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://Fquico1999.github.io/">Francisco Farinha</a></h5>
        <h6 class="card-subtitle">Data Scientist @ BlackBerry</h6>
        <p class="card-text">Interested in solving real-world problems using Machine Learning techniques.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/francisco-f/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Fquico1999" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


















    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      

      
      
      
    </div>
  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.8e6bf9dce3dc44c1896a95d36724b780.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    2024 Francisco Farinha | 
<a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic</a> | 
<a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>
  </p>

  
  






  
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
